{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!mv refcocog.tar.gz ./dataset/\n",
        "!ls dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q8rJi84feX2",
        "outputId": "3bc3a206-d733-4add-ce6d-647d70748cea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [05:59<00:00, 37.4MB/s]\n",
            "refcocog.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2vcCtcBfh9k",
        "outputId": "111c8aae-46bc-44ad-add6-96bdfeaefd35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "refcocog  refcocog.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "import sys\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "LdnLcbFyfkvW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZum46_RLQ44",
        "outputId": "e7a248a4-0609-47d6-aed0-901bdc88c865"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ng7x6gi7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ng7x6gi7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=74f41576f76d9f2f23ae33030f53b18be2b9efbd3daad77dae2473dc48a30ba5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4gj00n9s/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from clip import clip\n",
        "\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "model = model.to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4udizeKooQS0",
        "outputId": "e59f5ca9-ecb1-4966-8979-911319df1de2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 244M/244M [00:04<00:00, 62.5MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(images_fp: list[str], texts: list[str]):\n",
        "  # preprocess the images to transform from filenames to images to tensors\n",
        "  images = [preprocess(Image.open(image)) for image in images_fp]\n",
        "\n",
        "  # preprocess the texts to transform from text to tensors\n",
        "  images = torch.tensor(np.stack(images)).to(device)\n",
        "  text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "\n",
        "  # encode the inputs\n",
        "  with torch.no_grad():\n",
        "    images_z = model.encode_image(images).float().to(device)\n",
        "    texts_z = model.encode_text(text_tokens).float().to(device)\n",
        "  \n",
        "  return images_z, texts_z"
      ],
      "metadata": {
        "id": "didJ9Zi-F1dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (texts_z @ images_z.T)\n",
        "\n",
        "  return similarity.to(device)"
      ],
      "metadata": {
        "id": "TqNIHAq4F7aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "\n",
        "  \"\"\"A simple dataset representing the numbers from 0 to size-1\"\"\"\n",
        "\n",
        "  def __init__(self, root, transform=None, split=True, data_dir='dataset/refcocog'):\n",
        "    super(RefCOCOgDataset, self).__init__()\n",
        "    self.root = root = os.path.expanduser(root)\n",
        "    self.transform = transform\n",
        "    self.split = split\n",
        "\n",
        "    self.data = {}\n",
        "    f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "    self.data['refs'] = pickle.load(f)\n",
        "    instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "    instances = json.load(open(instances_file, 'r'))\n",
        "    self.data['images'] = instances['images']\n",
        "    self.data['annotations'] = instances['annotations']\n",
        "    print(self.data['refs'][0])\n",
        "    print(self.data['annotations'][0])\n",
        "    print(self.data['images'][0])\n",
        " \n",
        "    #return map(list, zip(*[self.preProcess_datasets(self.data['refs'], self.data['annotations'], self.data['images'])]))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"Get an item given its id.\n",
        "\n",
        "    Args:\n",
        "      idx: the integral index of the element to retrieve\n",
        "\n",
        "    Returns:\n",
        "      element at index idx\n",
        "    \"\"\"\n",
        "    result = torch.tensor([idx], dtype=torch.float32)\n",
        "\n",
        "    # if a transformation is available, we apply it\n",
        "    if self.transform is not None:\n",
        "      result = self.transform(result)\n",
        "    \n",
        "    # create train and test splits (80/20)\n",
        "    num_samples = len(result)\n",
        "    training_samples = int(num_samples * 0.8 + 1)\n",
        "    test_samples = num_samples - training_samples\n",
        "    if self.split:\n",
        "      training_data, test_data = torch.utils.data.random_split(result, [training_samples, test_samples])\n",
        "\n",
        "    return training_data, test_data\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Get the length of the dataset.\n",
        "\n",
        "    Returns:\n",
        "      number of elements that compose the dataset\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def preProcess_datasets(self, refs, anns, imgs):\n",
        "    images = []\n",
        "    texts = []       \n",
        "\n",
        "    for filename in imgs:\n",
        "      name = filename.stem\n",
        "      if name in refs:\n",
        "        for i, v in refs:\n",
        "          image = preprocess(Image.open(r'refcocog/images/'+v['file_name'])).unsqueeze(0).to(device)\n",
        "          for j, w in anns:\n",
        "            if(v['image_id'] == w['image_id']):\n",
        "              image = draw_bounding_boxes(image=image, boxes=w['bbox'], labels=v['sentences']['sent']''', fill=''').to(device)\n",
        "          image = torch.tensor(np.stack(image)).to(device)\n",
        "          text_token = clip.tokenize(v['sentences']['sent']).to(device)\n",
        "          image.shape, text_token.shape\n",
        "        images.append(image)\n",
        "        texts.append(text_token)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "\n",
        "    return images, texts "
      ],
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(root, batch_size=64, transform=True, test_batch_size=256):\n",
        "  \n",
        "    # build a chain of transformations\n",
        "    transformations_sequence = [\n",
        "      # random changes in pixel colors\n",
        "      T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "      # resize each PIL image to 256 x 256\n",
        "      T.Resize((256, 256)),                   \n",
        "      # the former transformations accept and return PIL Image objects, now convert to Tensor\n",
        "      T.ToTensor(),\n",
        "      # apply normalization\n",
        "      T.Normalize(mean=[0.4913, 0.4821, 0.4465], std=[0.2470, 0.2434, 0.2615])\n",
        "  ]\n",
        "    if not transform:\n",
        "      # convert the PIL images to Tensors\n",
        "      composed_transformation = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])  \n",
        "    else:\n",
        "      # prepare data transformations and then combine them sequentially\n",
        "      composed_transformation = T.Compose(transformations_sequence)\n",
        "\n",
        "    # load data\n",
        "    full_training_data, test_data = RefCOCOgDataset(root=root, transform=composed_transformation, split=True)\n",
        "\n",
        "    # create train and validation splits\n",
        "    num_samples = len(full_training_data)\n",
        "    training_samples = int(num_samples * 0.5 + 1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "    # initialize dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=4)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "    # normalise the image and the text\n",
        "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # evaluate the cosine similarity between the sets of features\n",
        "    similarity = (texts_z @ images_z.T)\n",
        "    return similarity.to(device)"
      ],
      "metadata": {
        "id": "n7OqJi3m809h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to training mode\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "    \n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ],
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.classifier.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C)\n",
        "  Output: (N, C)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm1d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm1d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm1d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C) to (C, N)\n",
        "    x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # during training keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      # no computational graph is necessary to be built for this computation\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.transpose(0, 1)"
      ],
      "metadata": {
        "id": "d0_wL4DDev2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C, H, W)\n",
        "  Output: (N, C, H, W)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm2d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm2d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4, 5, 5)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm2d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C, H, W) to (C, N, H, W)\n",
        "    x = x.transpose(0, 1)\n",
        "    \n",
        "    # store the shape\n",
        "    c, bs, h, w = x.shape\n",
        "    \n",
        "    # collapse all dimensions except the 'channel' dimension\n",
        "    x = x.contiguous().view(c, -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.view(c, bs, h, w).transpose(0, 1)\n"
      ],
      "metadata": {
        "id": "RJfcWScgfZGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 10):\n",
        "    super().__init__()\n",
        "    model, _ = clip.load(\"RN50\")\n",
        "\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "\n",
        "    # add a linear layer\n",
        "    self.classifier = torch.nn.Linear(1024, num_classes)\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.encoder(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "QjBOBP-Wvha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_zero_shot_clip(net, data_loader, texts_z, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval()\n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      # these two lines are different from the \"traditional\" ones\n",
        "      images_z = model.encode_image(inputs).float()\n",
        "      outputs = (100 * images_z @ texts_z.T).softmax(dim=-1)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += inputs.shape[0]\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_accuracy / samples * 100"
      ],
      "metadata": {
        "id": "T7ydDaNEwR-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleneckCLIP(torch.nn.Module):\n",
        "  def __init__(self, bias=False):\n",
        "    super().__init__()\n",
        "    model, _ = clip.load(\"RN50\")\n",
        "    in_features = 1024\n",
        "    out_features = 1024\n",
        "\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "\n",
        "    # add a bottleneck\n",
        "    self.bottleneck = torch.nn.Sequential([\n",
        "      torch.nn.Linear(in_features, in_features // 2, bias=bias),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, in_features // 2, bias=bias),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, out_features, bias=bias),\n",
        "    ])\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.encoder(x)\n",
        "    x = self.bottleneck(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ],
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=65, \n",
        "         root='/content/dataset/refcocog/'):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "  # instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(root=RefCOCOgDataset.data['refs'], batch_size=batch_size, transform=True, test_batch_size=None)\n",
        "\n",
        "  # pre-train model on zero-shot transfer learning\n",
        "  images, texts = map(list, zip(*[RefCOCOgDataset.preProcess_datasets(RefCOCOgDataset.data['refs'], RefCOCOgDataset.data['annotations'], RefCOCOgDataset.data['images'])]))\n",
        "  images_z, texts_z = encode_data(images, texts)\n",
        "  test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z)\n",
        "\n",
        "  # evaluate accuracy on zero-shot learning\n",
        "  print(cosine_similarity(images_z, texts_z))\n",
        "  print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "  # instantiate the network and move it to the chosen device (GPU)\n",
        "  modified_model = CustomCLIP(num_classes=num_classes).to(device)\n",
        "\n",
        "  # instantiate the optimizer\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "    \n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # closes the logger\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "RmcsfGIf_1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e852eaa7-497e-4b97-c99f-9a74ac0cfdab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1d58e722907c>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-1d58e722907c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(batch_size, device, learning_rate, weight_decay, momentum, epochs, num_classes, visualization_name, file_size, root)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# instantiates dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9b2fb32b62e7>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(root, file_size, batch_size, test_batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# prepare data transformations and then combine them sequentially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposed_transformation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# initialize dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-d3264c4cfc0f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# if a transformation is available, we apply it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# create train and test splits (80/20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1281\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontrast_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msaturation_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"contrast_factor ({contrast_factor}) is not non-negative.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0m_assert_image_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0m_assert_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36m_assert_image_tensor\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_image_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tensor_a_torch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor is not a torch image.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Tensor is not a torch image."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "CdKpBW-vPQfr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}