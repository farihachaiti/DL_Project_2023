{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SCUZq8R27Nv",
        "outputId": "b1dec353-afec-414c-8e76-2906144829f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "data_dir='/content/drive/MyDrive/ColabNotebooks/refcocog.tar.gz'\n",
        "# Extract data\n",
        "tar = tarfile.open(data_dir)\n",
        "tar.extractall('dataset/')\n",
        "\n",
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset"
      ],
      "metadata": {
        "id": "8Bum1NKU2v5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdnLcbFyfkvW"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install SummaryWriter\n",
        "!pip install tensorboard\n",
        "!pip install scikit-image\n",
        "!pip install matplotlib\n",
        "!pip install Lambda\n",
        "!pip install opencv-python\n",
        "\n",
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "torch.cuda.current_device()\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "import sys\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import torch\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "#from google.colab.patches import cv2_imshow\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "temperature = 1.0\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "data = {}\n",
        "to_pil = T.ToPILImage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZum46_RLQ44"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!cd yolov5\n",
        "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
        "! cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4udizeKooQS0"
      },
      "outputs": [],
      "source": [
        "from clip import clip\n",
        "\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "model = model.float()\n",
        "detector = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "outputs": [],
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "    def __init__(self, dataset, transform, target_transform, data_dir='dataset/refcocog'):\n",
        "        super(RefCOCOgDataset, self).__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.dataset = dataset\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_item = {}\n",
        "        fname = os.path.join(self.data_dir+'/images/', self.dataset[idx]['image'])\n",
        "        #fname = os.path.join(self.data_dir+'/images/', list(self.dataset.items())[idx][0])\n",
        "        image = Image.open(fname).convert('RGB')\n",
        "        #texts = list(self.dataset.items())[idx][1]\n",
        "        if self.transform:\n",
        "            data_item['image'] = self.dataset[idx]['image']\n",
        "        if self.target_transform:\n",
        "            data_item['captions'] = clip.tokenize([sent for desc in self.dataset[idx]['captions'] for sent in desc])\n",
        "        data_item['bbox'] = torch.Tensor(self.dataset[idx]['bbox'])\n",
        "        return data_item\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxhqoAtfkLaI"
      },
      "outputs": [],
      "source": [
        "def get_datasets(refs, train = True):\n",
        "  ref_data = []\n",
        "  for val in refs:\n",
        "    if train:\n",
        "      if val['split'] == 'train' or val['split'] == 'val':\n",
        "        ref_data.append(val)\n",
        "    else:\n",
        "      if val['split'] == 'test':\n",
        "        ref_data.append(val)\n",
        "\n",
        "  return ref_data\n",
        "\n",
        "\n",
        "def preProcess_datasets(data_dir, train):\n",
        "  dataset = {}\n",
        "  refs = {}\n",
        "  f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "  data['refs'] = pickle.load(f)\n",
        "  if train:\n",
        "    refs = get_datasets(data['refs'], train=True)\n",
        "  else:\n",
        "    refs = get_datasets(data['refs'], train=False)\n",
        "  instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "  instances = json.load(open(instances_file, 'r'))\n",
        "  data['images'] = instances['images']\n",
        "  data['annotations'] = instances['annotations']\n",
        "  print(data['refs'][0])\n",
        "  print(data['images'][0])\n",
        "  print(data['annotations'][0])\n",
        "  count=0\n",
        "  image_features = []\n",
        "  for key,val in enumerate(refs):\n",
        "    for v in data['images']:\n",
        "      if(val['image_id'] == v['id']):\n",
        "        fname = os.path.join(data_dir+'/images/', v['file_name'])\n",
        "        if os.path.exists(fname):\n",
        "          dataset[key] = {}\n",
        "          img = Image.open(fname).convert('RGB')\n",
        "          results = detector(img, size=640)\n",
        "          results = non_max_suppression(results.pandas().xyxy[0], 0.2, 0.2)\n",
        "          for det in results:\n",
        "            for *xyxy, conf, cls in det:\n",
        "              try:\n",
        "                t = modified_model.givemebox(img, xyxy)\n",
        "              except Exception as e:\n",
        "                continue\n",
        "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "              im_features.append(image_features)\n",
        "          im_features = torch.stack(im_features)\n",
        "          dataset[key]['image'] = im_features\n",
        "          dataset[key]['captions'] = [t['raw'] for t in val['sentences']]\n",
        "          dataset[key]['id'] = val['image_id']\n",
        "          break\n",
        "    for c in instances['annotations']:\n",
        "      if val['image_id']==c['image_id']:\n",
        "          dataset[key]['bbox'] = c['bbox']\n",
        "          break\n",
        "    count=count+1\n",
        "    if count==100:\n",
        "      break\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "  def visualise_result(img, txt, bbox):\n",
        "    resize_image = T.Resize(100)\n",
        "    img = resize_image(img)\n",
        "    convert_tensor = T.ToTensor()\n",
        "    image = convert_tensor(img).to(device)\n",
        "    image = image.clone().detach()\n",
        "    image = image.type(torch.ByteTensor).to(device)\n",
        "    boxs = torch.tensor(bbox,dtype=torch.int).to(device)\n",
        "    boxes = boxs.reshape([1,4])\n",
        "    boxes = torchvision.ops.box_convert(boxes, \"cxcywh\", \"xyxy\").to(device)\n",
        "    image = draw_bounding_boxes(image=image, boxes=boxes, width=2, colors=(0,0,255), fill=True).to(device)\n",
        "    image = image.permute(1,2,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_precision_recall(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    sorted_indices = np.argsort(pred_boxes[:, -1])[::-1]\n",
        "    pred_boxes = pred_boxes[sorted_indices]\n",
        "\n",
        "    num_predictions = len(pred_boxes)\n",
        "    num_targets = len(true_boxes)\n",
        "\n",
        "    true_positives = np.zeros(num_predictions)\n",
        "    false_positives = np.zeros(num_predictions)\n",
        "    false_negatives = np.zeros(num_targets)\n",
        "\n",
        "    for i, pred_box in enumerate(pred_boxes):\n",
        "        best_iou = 0\n",
        "        best_match_index = -1\n",
        "\n",
        "        for j, true_box in enumerate(true_boxes):\n",
        "            iou = calculate_iou1d(pred_box, true_box)\n",
        "\n",
        "            if iou > best_iou and iou >= iou_threshold:\n",
        "                best_iou = iou\n",
        "                best_match_index = j\n",
        "\n",
        "        if best_match_index >= 0:\n",
        "            true_positives[i] = 1\n",
        "            false_negatives[best_match_index] = 1\n",
        "        else:\n",
        "            false_positives[i] = 1\n",
        "\n",
        "    cum_true_positives = np.cumsum(true_positives)\n",
        "    cum_false_positives = np.cumsum(false_positives)\n",
        "    cum_false_negatives = np.cumsum(false_negatives)\n",
        "\n",
        "    precision = cum_true_positives / (cum_true_positives + cum_false_positives)\n",
        "    recall = cum_true_positives / (cum_true_positives + cum_false_negatives)\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "def calculate_ap(precision, recall):\n",
        "    recall = np.concatenate(([0], recall, [1]))\n",
        "    precision = np.concatenate(([0], precision, [0]))\n",
        "\n",
        "    for i in range(precision.size - 1, 0, -1):\n",
        "        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n",
        "\n",
        "    indices = np.where(recall[1:] != recall[:-1])[0] + 1\n",
        "    ap = np.sum((recall[indices] - recall[indices - 1]) * precision[indices])\n",
        "\n",
        "    return ap\n",
        "\n",
        "def calculate_mAP(ap_values):\n",
        "    mAP = np.mean(ap_values)\n",
        "    return mAP"
      ],
      "metadata": {
        "id": "mrdpbXp0MGTE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou1d(box1, box2):\n",
        "    # Calculate coordinates of intersection rectangle\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # Compute area of intersection\n",
        "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "\n",
        "    # Compute areas of the two bounding boxes\n",
        "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "    # Compute IoU\n",
        "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
        "\n",
        "    return iou\n"
      ],
      "metadata": {
        "id": "CXrLuQTtM7xv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def non_max_suppression(yolo_output, confidence_threshold, iou_threshold):\n",
        "    # Extract bounding box coordinates, class predictions, and confidence scores from the YOLO output\n",
        "\n",
        "    boxes = yolo_output.iloc[:,:4]\n",
        "    class_predictions = yolo_output.iloc[:,6]\n",
        "    confidence_scores = yolo_output.iloc[:,5]\n",
        "    selected_boxes = []\n",
        "    zipped = list(zip(boxes, class_predictions, confidence_scores))\n",
        "\n",
        "    df = pd.DataFrame(zipped)\n",
        "    for box, class_probs, confidence_scores in df.iterrows():\n",
        "        # Apply confidence thresholding\n",
        "        mask = confidence_scores >= confidence_threshold\n",
        "\n",
        "        class_probs = class_probs[mask]\n",
        "        confidence_scores = confidence_scores[mask]\n",
        "\n",
        "\n",
        "        # Apply non-maximum suppression\n",
        "        selected_indices = []\n",
        "        while len(confidence_scores) > 0:\n",
        "            # Select the box with the highest confidence score\n",
        "            best_index = np.argmax(confidence_scores)\n",
        "            selected_indices.append(best_index)\n",
        "\n",
        "            # Calculate IoU between the best box and the rest\n",
        "            ious = calculate_iou(box[best_index], box[selected_indices[:-1]])\n",
        "\n",
        "            # Remove indices of boxes with IoU greater than the threshold\n",
        "            indices_to_remove = [i for i, iou in enumerate(ious) if iou > iou_threshold]\n",
        "            box = np.delete(box, indices_to_remove, axis=0)\n",
        "            class_probs = np.delete(class_probs, indices_to_remove, axis=0)\n",
        "            confidence_scores = np.delete(confidence_scores.cpu(), indices_to_remove, axis=0)\n",
        "\n",
        "        # Collect the selected boxes along with class IDs and confidence scores\n",
        "        selected_boxes.extend([(b[0], b[1], b[2], b[3], np.argmax(c), s) for b, c, s in zip(box, class_probs, confidence_scores)])\n",
        "\n",
        "    return selected_boxes\n",
        "\n",
        "def calculate_iou(box1, boxes2):\n",
        "    x1 = np.maximum(box1[0], boxes2[0])\n",
        "    y1 = np.maximum(box1[1], boxes2[1])\n",
        "    x2 = np.minimum(box1[2], boxes2[2])\n",
        "    y2 = np.minimum(box1[3], boxes2[3])\n",
        "\n",
        "    intersection = np.maximum(0, x2 - x1 + 1) * np.maximum(0, y2 - y1 + 1)\n",
        "    area_box1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    area_boxes2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n",
        "    union = area_box1 + area_boxes2 - intersection\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou\n"
      ],
      "metadata": {
        "id": "lWxL54fhh_vU"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TqNIHAq4F7aM"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(tensor1: torch.Tensor, tensor2: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "\n",
        "  tensor1 /= tensor1.norm(dim=-1, keepdim=True)\n",
        "  tensor2 /= tensor2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  similarity = cos(tensor1, tensor2)\n",
        "  #similarity = (tensor1 @ tensor2.T)\n",
        "\n",
        "  return similarity.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.classifier.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d0_wL4DDev2a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C)\n",
        "  Output: (N, C)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm1d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm1d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm1d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "\n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "\n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # transpose (N, C) to (C, N)\n",
        "    x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
        "\n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "\n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "\n",
        "    # during training keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      # no computational graph is necessary to be built for this computation\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "\n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "\n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "\n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "\n",
        "    return x.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RJfcWScgfZGR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C, H, W)\n",
        "  Output: (N, C, H, W)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm2d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm2d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4, 5, 5)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm2d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "\n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "\n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # transpose (N, C, H, W) to (C, N, H, W)\n",
        "    x = x.transpose(0, 1)\n",
        "\n",
        "    # store the shape\n",
        "    c, bs, h, w = x.shape\n",
        "\n",
        "    # collapse all dimensions except the 'channel' dimension\n",
        "    x = x.contiguous().view(c, -1)\n",
        "\n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "\n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "\n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "\n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "\n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "\n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "\n",
        "    return x.view(c, bs, h, w).transpose(0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # set the network to training mode\n",
        "    net = net.float()\n",
        "    net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "      img = batch['image']\n",
        "      bbox = batch['bbox']\n",
        "      label = batch['captions']\n",
        "      # forward pass\n",
        "      outputs = net(batch)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, bbox)\n",
        "      #print('loss')\n",
        "      #print(loss)\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # parameters update\n",
        "      optimizer.step()\n",
        "\n",
        "      # gradients reset\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      #samples += batch['image'].shape[0]'\n",
        "\n",
        "      loss = cost_function(outputs, bbox)\n",
        "      samples += img.shape[0]\n",
        "      cumulative_loss += loss\n",
        "      cIoU = calculate_iou(outputs, bbox)\n",
        "      cumulative_cIoU += cIoU.sum().item()\n",
        "      cumulative_count += len(outputs)\n",
        "      # print(cumulative_cIoU)\n",
        "      # print(cumulative_count)\n",
        "      torch.cuda.empty_cache()\n",
        "      samples += len(outputs)\n",
        "      cumulative_loss += loss.item()\n",
        "\n",
        "      cIoU = calculate_iou(outputs, bbox)\n",
        "      cumulative_cIoU += cIoU.sum().item()\n",
        "      cumulative_count += len(outputs)\n",
        "      # print(cumulative_cIoU)\n",
        "      # print(cumulative_count)\n",
        "      torch.cuda.empty_cache()\n",
        "    return cumulative_loss / samples, cumulative_cIoU / cumulative_count\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    net = net.float()\n",
        "    # set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, batch in enumerate(data_loader):\n",
        "        img = batch['image']\n",
        "        bbox = batch['bbox']\n",
        "        label = batch['captions']\n",
        "        outputs = net(batch)\n",
        "        loss = cost_function(outputs, bbox)\n",
        "\n",
        "        samples += img.shape[0]\n",
        "        cumulative_loss += loss\n",
        "\n",
        "        cIoU = calculate_iou(outputs, bbox)\n",
        "        cumulative_cIoU += cIoU.sum().item()\n",
        "        cumulative_count += len(outputs)\n",
        "        # print(cumulative_cIoU)\n",
        "        # print(cumulative_count)\n",
        "        torch.cuda.empty_cache()\n",
        "    return cumulative_loss / samples, cumulative_cIoU / cumulative_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vd2n2ryW5fIc"
      },
      "outputs": [],
      "source": [
        "class ProjectionHead(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        in_features = 1024,\n",
        "        out_features = 1024,\n",
        "        bias=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "          torch.nn.Linear(in_features, out_features, bias=bias),\n",
        "          torch.nn.GELU(),\n",
        "          torch.nn.Linear(out_features, out_features, bias=bias),\n",
        "          torch.nn.Dropout(0.1),\n",
        "          torch.nn.BatchNorm1d(1024),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 10, bias=False):\n",
        "    super().__init__()\n",
        "    self.detector = detector\n",
        "    self.model = model\n",
        "    in_features = 1024\n",
        "    out_features = 1024\n",
        "    embedding_dim = ''\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "    self.text_encoder = self.encode_text\n",
        "\n",
        "    self.projection = ProjectionHead(embedding_dim)\n",
        "    #self.classifier = torch.nn.Linear(1024, num_classes)\n",
        "    # add a bottleneck\n",
        "    self.classifier = torch.nn.Sequential(\n",
        "      torch.nn.Linear(in_features, out_features, bias=bias),\n",
        "      torch.nn.GELU(),\n",
        "      torch.nn.Linear(out_features, out_features, bias=bias),\n",
        "      torch.nn.Dropout(0.1),\n",
        "      torch.nn.BatchNorm1d(1024),\n",
        "    )\n",
        "\n",
        "    def givemebox(self, img, bbox):\n",
        "      x1 = int(bbox[0])\n",
        "      y1 = int(bbox[1])\n",
        "      x2 = int(bbox[2])\n",
        "      y2 = int(bbox[3])\n",
        "      t = torch.stack((img[0][0][y1:y2, x1:x2], img[0][1][y1:y2, x1:x2], img[0][2][y1:y2, x1:x2]))\n",
        "      t = to_pil(t)\n",
        "      t = preprocess(t).unsqueeze(0).to(device)\n",
        "      return t\n",
        "\n",
        "  def encode_text(self, text):\n",
        "      x = model.token_embedding(text).float()\n",
        "      x = x + model.positional_embedding.float()\n",
        "      x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "      #x = x.type(torch.float16).to(device)\n",
        "      x = model.transformer(x).float()\n",
        "      x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "      x = model.ln_final(x).float()\n",
        "\n",
        "      # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "      # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "      x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ model.text_projection\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  def forward(self, batch):\n",
        "    similar = []\n",
        "    '''(batch['image'].shape)\n",
        "    clip_image = to_pil(np.squeeze(batch['image']))\n",
        "\n",
        "    im_features = []\n",
        "    results = self.detector(clip_image, size=640)\n",
        "    results = non_max_suppression(results, 0.2, 0.2)\n",
        "    for det in results:\n",
        "      for *xyxy, conf, cls in det:\n",
        "        try:\n",
        "          t = self.givemebox(clip_image, xyxy)\n",
        "        except Exception as e:\n",
        "          continue\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(t)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        im_features.append(image_features)\n",
        "    im_features = torch.stack(im_features)'''\n",
        "    image_features = self.classifier(batch['image'])\n",
        "    captions = batch['captions'].to(device)\n",
        "    y = self.text_encoder(captions)\n",
        "    text_features = self.classifier(y)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    #target_len = y.size(0) - x.size(0)\n",
        "    #x = F.pad(x, (0,0,0,target_len), value=0)\n",
        "    #similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    #values, indices = similarity[0].topk(1)\n",
        "    '''image_features_similarity = image_features @ image_features.T\n",
        "    text_features_similarity = text_features @ text_features.T\n",
        "\n",
        "\n",
        "    targets = F.softmax(\n",
        "      (image_features_similarity + text_features_similarity) / 2 * temperature, dim=-1\n",
        "    )\n",
        "\n",
        "\n",
        "    outputs =  (image_features_similarity @ text_features_similarity.T) / temperature'''\n",
        "\n",
        "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=0)\n",
        "    top_values, top_indices = similarity.topk(1, dim=0)\n",
        "\n",
        "    return results[top_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ag-CVvdzvIDN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class IoULoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred_boxes, target_boxes):\n",
        "        batch_size, num_boxes = pred_boxes.size()\n",
        "\n",
        "        iou = calculate_iou(pred_boxes, target_boxes)\n",
        "        loss = 1 - iou\n",
        "        return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "glR14_FUW3ds"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  loss = IoULoss()\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-VCNbFs5W24g"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "outputs": [],
      "source": [
        "def pad_sequence(batch):\n",
        "  return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "def my_collate_fn(batch):\n",
        "\n",
        "  return {\n",
        "      'image': torch.stack([x['image'] for x in batch]),\n",
        "      'captions': torch.cat([x['captions'] for x in batch]),\n",
        "      'bbox' : torch.stack([x['bbox'] for x in batch])\n",
        "      }\n",
        "\n",
        "\n",
        "def get_data(data_dir, batch_size, transform=True, target_transform=False, test_batch_size=15):\n",
        "\n",
        "\n",
        "    if transform:\n",
        "        # convert the PIL images to Tensors\n",
        "        transform = preprocess\n",
        "    else:\n",
        "          # prepare data transformations and then combine them sequentially\n",
        "        transform = T.Compose([torchvision.transforms.ToTensor()])\n",
        "    if target_transform:\n",
        "        target_transform = T.Compose([\n",
        "                                 lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)])\n",
        "    else:\n",
        "        target_transform = None\n",
        "\n",
        "  # load data\n",
        "    dataset = preProcess_datasets(data_dir, train=True)\n",
        "    full_training_data = RefCOCOgDataset(dataset=dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
        "\n",
        "    test_dataset = preProcess_datasets(data_dir, train=False)\n",
        "    test_data = RefCOCOgDataset(dataset=test_dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
        "\n",
        "\n",
        "\n",
        "    evens = list(range(0, len(full_training_data), 2))\n",
        "    #training_data2 = torch.utils.data.Subset(full_training_data, evens)\n",
        "    training_data2 = torch.utils.data.Subset(full_training_data, range(50))\n",
        "    test_data2 = torch.utils.data.Subset(test_data, range(50))\n",
        "\n",
        "  # create train and validation splits\n",
        "    num_samples = len(training_data2)\n",
        "    training_samples = int(num_samples * 0.5 + 1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    training_data, validation_data = torch.utils.data.random_split(training_data2, [training_samples, validation_samples])\n",
        "\n",
        "  # initialize dataloaders_collate\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=0, collate_fn=my_collate_fn)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=0, collate_fn=my_collate_fn)\n",
        "    #test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data2, test_batch_size, shuffle=False, num_workers=0, collate_fn=my_collate_fn)\n",
        "\n",
        "    print(len(training_data))\n",
        "\n",
        "    print(len(validation_data))\n",
        "    print(len(test_data))\n",
        "\n",
        "    # pre-train model on zero-shot transfer learning\n",
        "    #test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z)\n",
        "\n",
        "\n",
        "    # evaluate accuracy on zero-shot learning\n",
        "\n",
        "    #print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "outputs": [],
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k6UDZHcZksAv"
      },
      "outputs": [],
      "source": [
        "modified_model = CustomCLIP(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RmcsfGIf_1fd"
      },
      "outputs": [],
      "source": [
        "# main funcition\n",
        "def main(\n",
        "      root='/content/dataset/refcocog/',\n",
        "      data_dir='dataset/refcocog',\n",
        "      batch_size=2,\n",
        "      num_classes=10,\n",
        "      learning_rate=0.000000000001,\n",
        "      weight_decay=0.000001,\n",
        "      momentum=0.9,\n",
        "      #epochs=10,\n",
        "      epochs=3\n",
        "    ):\n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "  global test_loader\n",
        "  # train clip on zero-shot learning and instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, target_transform=True, test_batch_size=2)\n",
        "  #train_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, target_transform=True, test_batch_size=32)\n",
        "  #dataset = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, test_batch_size=None)\n",
        "\n",
        "  # instantiate the network and move it to the chosen device (GPU)\n",
        "\n",
        "\n",
        "  # instantiate the optimizer\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # evaluate accuracy of modified model on zero-shot learning\n",
        "  #test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z).to(device)\n",
        "\n",
        "  #print(cosine_similarity(images_z, texts_z))\n",
        "  #print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "  # computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "\n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  # closes the logger\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7IXV0biKmV1u"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<1024>\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_S2MFs8VJER",
        "outputId": "79faf38c-fbe5-4d1a-e233-9841b2acd644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_accuracy(output, target):\n",
        "\n",
        "\n",
        "  f1_scr = f1_score(target, output, average='weighted')\n",
        "\n",
        "  cm = confusion_matrix(target, output)\n",
        "  TP = cm[0][0]\n",
        "  FP = cm[0][1]\n",
        "  FN = cm[1][0]\n",
        "  TN = cm[1][1]\n",
        "\n",
        "  #accuracy = np.sum(np.diagonal(cm)) / np.sum(cm)\n",
        "  accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "CdKpBW-vPQfr",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "49c98e57-e3e1-4535-9f28-0c5098b3e0a7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-189-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-08f20d587c45>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(root, data_dir, batch_size, num_classes, learning_rate, weight_decay, momentum, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mglobal\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# train clip on zero-shot learning and instantiates dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0;31m#train_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, target_transform=True, test_batch_size=32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m#dataset = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, test_batch_size=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-0e608fdbfb61>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(data_dir, batch_size, transform, target_transform, test_batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcess_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mfull_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-dda63c4199b1>\u001b[0m in \u001b[0;36mpreProcess_datasets\u001b[0;34m(data_dir, train)\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-188-69bf00407557>\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[0;34m(yolo_output, confidence_threshold, iou_threshold)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence_scores\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Apply confidence thresholding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfidence_scores\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconfidence_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roiPJi88dzRt"
      },
      "outputs": [],
      "source": [
        "#!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
        "#im = cv2.imread(\"./input.jpg\")\n",
        "def draw_bbx(im):\n",
        "  #!python yolov5/classify/predict.py --weights yolov5s.pt --source im --view-img\n",
        "  results = detector(im)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTW32KCDQvVJ"
      },
      "outputs": [],
      "source": [
        "def get_img_embeddings(loader, model):\n",
        "\n",
        "  test_image_embeddings = []\n",
        "\n",
        "  for idx, batch in enumerate(loader):\n",
        "    batch['image'] = batch['image'].to(device)\n",
        "    image_features = model.encoder(batch['image'])\n",
        "    image_embeddings = model.projection(image_features)\n",
        "    test_image_embeddings.append(image_embeddings)\n",
        "\n",
        "  test_image_embeddings = torch.cat(test_image_embeddings)\n",
        "\n",
        "  return test_image_embeddings\n",
        "\n",
        "\n",
        "def inference(model, query, image_filenames, n):\n",
        "\n",
        "    model.eval()\n",
        "    global image_embeddings\n",
        "\n",
        "    tokenized_query = clip.tokenize([query]).to(device)\n",
        "    '''batch = {\n",
        "        key: torch.tensor(values).to(device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }'''\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      image_embeddings = get_img_embeddings(test_loader, model)\n",
        "      text_features = model.text_encoder(tokenized_query)\n",
        "      text_embeddings = model.projection(text_features)\n",
        "\n",
        "      image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "      text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "      dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "\n",
        "      #print(dot_similarity.squeeze(0))\n",
        "      val, index = torch.topk(dot_similarity.squeeze(0), n)\n",
        "      found_match = image_filenames[index]\n",
        "\n",
        "      plt.figure(1)\n",
        "    #for match, ax in zip(matches, axes.flatten()):\n",
        "      image = cv2.imread(f\"{'dataset/refcocog/images'}/{found_match['file_name']}\")\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      results = draw_bbx(image)\n",
        "      results.show()\n",
        "      '''imgplot = plt.imshow(image)\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "      #draw_bbx(image)'''\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzgeqEvWmnfv"
      },
      "outputs": [],
      "source": [
        "inference(modified_model,\n",
        "             query=\"tree\",\n",
        "             image_filenames=data['images'], n=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8-_Wjd-USfm"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "to_pil = transforms.ToPILImage()\n",
        "def baseline(x):\n",
        "    image = x[0]\n",
        "    captions = x[1]\n",
        "    result = yolo(image)\n",
        "    bboxes = result.crop(save=False)\n",
        "    im_features = []\n",
        "    for bbox in bboxes:\n",
        "        image_input = preprocess(Image.fromarray(bbox['im'])).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image_input)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        im_features.append(image_features)\n",
        "    if len(bboxes) == 0:\n",
        "      return 0, torch.zeros(4)\n",
        "    text_inputs = captions.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_inputs)\n",
        "\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    im_features = torch.stack(im_features)\n",
        "    similarity = (100.0 * im_features @ text_features.T).softmax(dim=0)\n",
        "    top_values, top_indices = similarity.topk(1, dim=0)\n",
        "    return top_values.item(), bboxes[top_indices]['box']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Jxf117sxNm"
      },
      "outputs": [],
      "source": [
        "pred_boxes = []\n",
        "true_boxes = []\n",
        "\n",
        "for idx, x in enumerate(dataset):\n",
        "  print(f\"\\nTop predictions for {dataset.descriptions[idx][0]['raw']}:-------------------------\\n\")\n",
        "  conf, box = baseline(x)\n",
        "  print(\"target:\", x[2])\n",
        "  print(\"confidence: \", 100*conf)\n",
        "  print(\"box: \", box)\n",
        "  if conf != 0:\n",
        "    x1,y1, x2, y2 = box\n",
        "    pred_boxes.append([x1.item(), y1.item(), x2.item(), y2.item()])\n",
        "    true_boxes.append(x[2].tolist())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}