{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q8rJi84feX2",
        "outputId": "b4adacc2-0b1a-40ea-ccba-a267a965a417",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq \n",
            "\n",
            "mv: cannot stat 'refcocog.tar.gz': No such file or directory\n",
            "refcocog\n"
          ]
        }
      ],
      "source": [
        "!mkdir dataset\n",
        "!pip install gdown\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!mv refcocog.tar.gz ./dataset/\n",
        "!ls dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "     \n",
        "\n",
        "import tarfile\n",
        "data_dir='/content/drive/MyDrive/refcocog.tar.gz'\n",
        "# Extract data\n",
        "tar = tarfile.open(data_dir)\n",
        "tar.extractall('dataset/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjQevTnJKfRE",
        "outputId": "01c97a00-4256-4c6e-865e-ce1f7c31442b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2vcCtcBfh9k",
        "outputId": "6dfb14c0-6384-4294-9816-7519ea28c30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: dataset/refcocog.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "refcocog\n"
          ]
        }
      ],
      "source": [
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdnLcbFyfkvW",
        "outputId": "98b2f3ee-6c85-4ba0-f4df-9a244cf1f88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement SummaryWriter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for SummaryWriter\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Lambda in /usr/local/lib/python3.10/dist-packages (0.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install SummaryWriter\n",
        "!pip install tensorboard\n",
        "!pip install scikit-image\n",
        "!pip install matplotlib\n",
        "!pip install Lambda\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "import sys\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import torch\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "temperature = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZum46_RLQ44",
        "outputId": "4f4b8dfa-1773-4982-b14e-240abbc12c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-djsjca5b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-djsjca5b\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4udizeKooQS0"
      },
      "outputs": [],
      "source": [
        "from clip import clip\n",
        "\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "model = model.float().to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "outputs": [],
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "    def __init__(self, dataset, transform, target_transform, data_dir='dataset/refcocog'):\n",
        "        super(RefCOCOgDataset, self).__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.dataset = dataset\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        data_item =  \n",
        "        { key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()}\n",
        "        fname = os.path.join(self.data_dir+'/images/', self.dataset[idx]['image'])\n",
        "        #fname = os.path.join(self.data_dir+'/images/', list(self.dataset.items())[idx][0])\n",
        "        image = Image.open(fname).convert('RGB')\n",
        "        #texts = list(self.dataset.items())[idx][1]\n",
        "        if self.transform:\n",
        "            data_item['image'] = self.transform(image).float()\n",
        "        if self.target_transform:\n",
        "            data_item['captions'] = [sent for desc in self.dataset[idx]['captions'] for sent in desc]\n",
        "        return data_item\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxhqoAtfkLaI"
      },
      "outputs": [],
      "source": [
        "def get_datasets(refs, train = True):\n",
        "  ref_data = []\n",
        "  for val in refs:\n",
        "    if train:\n",
        "      if val['split'] == 'train' or val['split'] == 'val':\n",
        "        ref_data.append(val)\n",
        "    else:\n",
        "      if val['split'] == 'test':\n",
        "        ref_data.append(val)\n",
        "  \n",
        "  '''for i,v in enumerate(datasets):\n",
        "    training_data, test_data = v'''\n",
        "\n",
        "  return ref_data\n",
        "\n",
        "\n",
        "def preProcess_datasets(data_dir, train):\n",
        "  dataset = {}\n",
        "  data = {}\n",
        "  refs = {}     \n",
        "  f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "  data['refs'] = pickle.load(f)\n",
        "  if train:\n",
        "    refs = get_datasets(data['refs'], train=True)\n",
        "  else:\n",
        "    refs = get_datasets(data['refs'], train=False)\n",
        "  instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "  instances = json.load(open(instances_file, 'r'))\n",
        "  data['images'] = instances['images']\n",
        "  data['annotations'] = instances['annotations'] \n",
        "  #print(data['refs'][0])\n",
        "  #print(data['images'][0])\n",
        "  #print(data['annotations'][0])\n",
        "  for key,val in enumerate(refs):\n",
        "    for v in data['images']:\n",
        "      if(val['image_id'] == v['id']): \n",
        "        fname = os.path.join(data_dir+'/images/', v['file_name'])\n",
        "        if os.path.exists(fname):\n",
        "          dataset[key] = {}\n",
        "          dataset[key]['image'] = v['file_name']\n",
        "          dataset[key]['captions'] = [t['sent'] for t in val['sentences']]\n",
        "          dataset[key]['id'] = val['image_id']\n",
        "      else: \n",
        "        continue\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "  def visualise_result(img, txt):\n",
        "    resize_image = T.Resize(100)\n",
        "    img = resize_image(img)\n",
        "    convert_tensor = T.ToTensor()\n",
        "    image = convert_tensor(img).to(device)\n",
        "    image = image.clone().detach()\n",
        "    image = image.type(torch.ByteTensor).to(device)\n",
        "    boxs = torch.tensor(w['bbox'],dtype=torch.int).to(device) \n",
        "    boxes = boxs.reshape([1,4])   \n",
        "    boxes = torchvision.ops.box_convert(boxes, \"cxcywh\", \"xyxy\").to(device)\n",
        "    image = draw_bounding_boxes(image=image, boxes=boxes, width=2, colors=(0,0,255), fill=True).to(device)\n",
        "    image = image.permute(1,2,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "didJ9Zi-F1dN"
      },
      "outputs": [],
      "source": [
        "def encode_data(batch):\n",
        "    # preprocess the images to transform from filenames to images to tensors\n",
        "    '''images = [preprocess(Image.open(os.path.join(data_dir+'/images/',image))) for image in images_fp]\n",
        "\n",
        "    # preprocess the texts to transform from text to tensors\n",
        "    images = torch.tensor(np.stack(images)).to(device)\n",
        "\n",
        "    text_tokens = clip.tokenize([\"This is \" + sent for desc in texts for sent in desc]).to(device)\n",
        "\n",
        "    # encode the inputs\n",
        "    with torch.no_grad():\n",
        "        images_z = model.encode_image(images).float()\n",
        "        texts_z = model.encode_text(text_tokens).float()\n",
        "        texts_z /= texts_z.norm(dim=-1, keepdim=True)'''\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "      #for batch_idx, batch in enumerate(data_loader):\n",
        "      image_input = batch['image']\n",
        "      text_input = batch['labels']\n",
        "    # load data into GPU\n",
        "      image_input = image_input.to(device)\n",
        "      text_input = text_input.to(device)\n",
        "      text_features_all = []\n",
        "            \n",
        "      text_features = model.encode_text(text_input).float()\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "      text_features_all.append(text_features)\n",
        "\n",
        "      # evaluate the mean representation\n",
        "      text_features = torch.stack(text_features_all).mean(dim=0)\n",
        "\n",
        "      # renormalise\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # forward pass\n",
        "    # these two lines are different from the \"traditional\" ones\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    return image_features, text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqNIHAq4F7aM"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(tensor1: torch.Tensor, tensor2: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  tensor1 /= tensor1.norm(dim=-1, keepdim=True)\n",
        "  tensor2 /= tensor2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (tensor1 @ tensor2.T)\n",
        "\n",
        "  return similarity.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # set the network to training mode\n",
        "    net = net.float()\n",
        "    net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        \n",
        "      #load data into GPU\n",
        "      batch['image'] = batch['image'].to(device)\n",
        "      batch['captions'] = batch['captions'].to(device)\n",
        "        # forward pass\n",
        "      outputs, targets = net(batch)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # parameters update\n",
        "      optimizer.step()\n",
        "\n",
        "      # gradients reset\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += batch['image'].shape[0]\n",
        "      cumulative_loss += loss.item()\n",
        "      _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "      # compute training accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    net = net.float()\n",
        "    # set the network to evaluation mode\n",
        "    net.eval() \n",
        "\n",
        "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "      \n",
        "      for batch_idx, batch in enumerate(data_loader):\n",
        "      \n",
        "        #load data into GPU\n",
        "        batch['image'] = batch['image'].to(device)\n",
        "        batch['captions'] = batch['captions'].to(device)\n",
        "        \n",
        "        # forward pass\n",
        "        outputs, targets = net(batch)\n",
        "\n",
        "        # loss computation\n",
        "        loss = cost_function(outputs, targets)\n",
        "        # fetch prediction and loss value\n",
        "        samples += batch['image'].shape[0]\n",
        "        cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "        _, predicted = outputs.max(1)\n",
        "\n",
        "        # compute accuracy\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.classifier.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "            \n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # we are using the CLS token hidden representation as the sentence's embedding\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]"
      ],
      "metadata": {
        "id": "b4rgJAw9ZyJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0_wL4DDev2a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C)\n",
        "  Output: (N, C)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm1d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm1d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm1d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C) to (C, N)\n",
        "    x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # during training keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      # no computational graph is necessary to be built for this computation\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJfcWScgfZGR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C, H, W)\n",
        "  Output: (N, C, H, W)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm2d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm2d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4, 5, 5)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm2d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C, H, W) to (C, N, H, W)\n",
        "    x = x.transpose(0, 1)\n",
        "    \n",
        "    # store the shape\n",
        "    c, bs, h, w = x.shape\n",
        "    \n",
        "    # collapse all dimensions except the 'channel' dimension\n",
        "    x = x.contiguous().view(c, -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.view(c, bs, h, w).transpose(0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7ydDaNEwR-2"
      },
      "outputs": [],
      "source": [
        "def test_step_zero_shot_clip(net, data_loader, texts_z):\n",
        "    samples = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "            image_input = batch['image']\n",
        "            targets = batch['labels']\n",
        "          # load data into GPU\n",
        "            image_input = image_input.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            text_features_all = []\n",
        "                  \n",
        "            with torch.no_grad():\n",
        "              text_features = model.encode_text(texts_z).float()\n",
        "              text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "              text_features_all.append(text_features)\n",
        "    \n",
        "            # evaluate the mean representation\n",
        "            text_features = torch.stack(text_features_all).mean(dim=0)\n",
        "\n",
        "            # renormalise\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "          # forward pass\n",
        "          # these two lines are different from the \"traditional\" ones\n",
        "            image_features = model.encode_image(image_input).float()\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "   \n",
        "            outputs = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "            # fetch prediction and loss value\n",
        "            samples += image_input.shape[0]\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "          # compute accuracy\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "            print(cosine_similarity(images_z, texts_z))\n",
        "            \n",
        "    return cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 10, bias=False):\n",
        "    super().__init__()\n",
        "    #model, _ = clip.load(\"RN50\")\n",
        "    in_features = 1024\n",
        "    out_features = 1024\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "    #self.text_encoder = self.encode_text\n",
        "    self.text_encoder = TextEncoder()\n",
        "\n",
        "    #self.classifier = torch.nn.Linear(1024, num_classes)   \n",
        "    # add a bottleneck\n",
        "    self.classifier = torch.nn.Sequential(\n",
        "      torch.nn.Linear(in_features, in_features // 2, bias=bias),\n",
        "      torch.nn.BatchNorm1d(512),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, in_features // 2, bias=bias),\n",
        "      torch.nn.BatchNorm1d(512),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, out_features, bias=bias),\n",
        "      torch.nn.BatchNorm1d(1024),\n",
        "      torch.nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def encode_text(self, text):\n",
        "      x = model.token_embedding(text).float()\n",
        "      x = x + model.positional_embedding.float()\n",
        "      x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "      #x = x.type(torch.float16).to(device)\n",
        "      x = model.transformer(x).float()\n",
        "      x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "      x = model.ln_final(x).float()\n",
        "\n",
        "      # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "      # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "      x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ model.text_projection\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "  def forward(self, batch): \n",
        "    x = batch['image']\n",
        "    x = self.encoder(x)\n",
        "    x = self.classifier(x)\n",
        "    y = batch['captions']\n",
        "    y = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "    y = self.text_encoder(y)\n",
        "    y = self.classifier(y)\n",
        "\n",
        "    target_len = y.size(0) - x.size(0)\n",
        "    x = F.pad(x, (0,0,0,target_len), value=0)\n",
        "    x_similarity = cosine_similarity(x,x)\n",
        "    y_similarity = cosine_similarity(y,y)\n",
        "\n",
        "\n",
        "    targets = F.softmax(\n",
        "      (x_similarity + y_similarity) / 2 * temperature, dim=-1\n",
        "    )\n",
        "\n",
        "    target= torch.argmax(targets ,axis=1)\n",
        "    outputs = cosine_similarity(x,y)\n",
        "\n",
        "\n",
        "    return outputs, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "outputs": [],
      "source": [
        "def pad_sequence(batch):\n",
        "  return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "def my_collate_fn(batch):\n",
        "  '''captions = captions.squeeze(0).detach().cpu().numpy()\n",
        "  captions = captions[:,:,0]\n",
        "  print(captions.shape)\n",
        "  captions = captions.permute(2,1,0)\n",
        "  print(captions.shape)\n",
        "  captions = captions.float().mean(2)'''\n",
        "  return {\n",
        "      'image': torch.stack([x['image'] for x in batch]),\n",
        "      'captions': torch.cat([x['captions'] for x in batch], dim=0)\n",
        "      }\n",
        "\n",
        "\n",
        "def get_data(data_dir, batch_size=32, transform=True, target_transform=True, test_batch_size=32):\n",
        "\n",
        "  \n",
        "    if transform:\n",
        "        # convert the PIL images to Tensors\n",
        "        transform = preprocess\n",
        "    else:\n",
        "          # prepare data transformations and then combine them sequentially\n",
        "        transform = T.Compose([torchvision.transforms.ToTensor()]) \n",
        "    if target_transform:   \n",
        "        target_transform = T.Compose([\n",
        "                                 lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)])\n",
        "    else:\n",
        "        target_transform = None\n",
        "\n",
        "  # load data\n",
        "    dataset = preProcess_datasets(data_dir, train=True)\n",
        "    full_training_data = RefCOCOgDataset(dataset=dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
        "\n",
        "    test_dataset = preProcess_datasets(data_dir, train=False)\n",
        "    test_data = RefCOCOgDataset(dataset=test_dataset, transform=transform, target_transform=target_transform, data_dir=data_dir)\n",
        "  \n",
        "\n",
        "\n",
        "    evens = list(range(0, len(full_training_data), 2))\n",
        "    training_data2 = torch.utils.data.Subset(full_training_data, evens)\n",
        "\n",
        "\n",
        "  # create train and validation splits\n",
        "    num_samples = len(training_data2)\n",
        "    training_samples = int(num_samples * 0.5 + 1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    training_data, validation_data = torch.utils.data.random_split(training_data2, [training_samples, validation_samples])\n",
        "\n",
        "  # initialize dataloaders_collate\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=2, collate_fn=my_collate_fn)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
        "\n",
        "    print(len(training_data))\n",
        "\n",
        "    print(len(validation_data))\n",
        "    print(len(test_data))\n",
        "\n",
        "    # pre-train model on zero-shot transfer learning\n",
        "    #test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z)\n",
        "\n",
        "\n",
        "    # evaluate accuracy on zero-shot learning\n",
        "\n",
        "    #print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "outputs": [],
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmcsfGIf_1fd"
      },
      "outputs": [],
      "source": [
        "# main funcition\n",
        "def main(\n",
        "      root='/content/dataset/refcocog/',\n",
        "      data_dir='dataset/refcocog',\n",
        "      batch_size=32,\n",
        "      num_classes=10,\n",
        "      learning_rate=0.01,\n",
        "      weight_decay=0.000001,\n",
        "      momentum=0.9,\n",
        "      epochs=10,\n",
        "    ): \n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "  # train clip on zero-shot learning and instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, target_transform=True, test_batch_size=32)\n",
        "  #dataset = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, test_batch_size=None)\n",
        "\n",
        "  # instantiate the network and move it to the chosen device (GPU)\n",
        "  modified_model = CustomCLIP(num_classes=num_classes).to(device)\n",
        "  \n",
        "  # instantiate the optimizer\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # evaluate accuracy of modified model on zero-shot learning\n",
        "  #test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z).to(device)\n",
        "  \n",
        "  #print(cosine_similarity(images_z, texts_z))\n",
        "  #print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "  # computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "    \n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  # closes the logger\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IXV0biKmV1u"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<1024>\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdKpBW-vPQfr",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25795ae5-d165-465a-ed2c-1810c8c7e5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11201\n",
            "11199\n",
            "5023\n",
            "Before training:\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "data = torch.ones(4, 5)\n",
        "data2 = torch.zeros(5,6)\n",
        "print(data.shape)\n",
        "#len = data2.size(0)-data.size(0)\n",
        "# pad(left, right, top, bottom)\n",
        "data = F.pad(data, (0,0,0,1),  value=0)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "1nK6z3JEO9IM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264d5ae5-d7e9-4d26-dacd-95255034c003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 5])\n",
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}