{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!mv refcocog.tar.gz ./dataset/\n",
        "!ls dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q8rJi84feX2",
        "outputId": "8c7d8b60-0c95-4e14-8381-6f5948fb961b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [04:27<00:00, 50.3MB/s]\n",
            "refcocog.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset"
      ],
      "metadata": {
        "id": "P2vcCtcBfh9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "import sys\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "LdnLcbFyfkvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZum46_RLQ44",
        "outputId": "e9046100-1863-4cba-f4c6-4da241b373d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ba5vjhmc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ba5vjhmc\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=9f0c83c27f118004ab4045db8a70c274a71fa43c8b350ae724bda27939a03ec0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fw_klse5/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from clip import clip\n",
        "\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "model = model.to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4udizeKooQS0",
        "outputId": "f6aa945e-10c2-43f8-f309-0871bb891b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 244M/244M [00:01<00:00, 174MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "\n",
        "  data = {}\n",
        "  training_data = []\n",
        "  test_data = []\n",
        "  def __init__(self, root, transform=None, data_dir='dataset/refcocog'):\n",
        "    super(RefCOCOgDataset, self).__init__()\n",
        "    #self.data = data\n",
        "    self.transform = transform\n",
        "    self.root = root = os.path.expanduser(root+'images/') \n",
        "    f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "    self.data['refs'] = pickle.load(f)\n",
        "    instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "    instances = json.load(open(instances_file, 'r'))\n",
        "    self.data['images'] = instances['images']\n",
        "    self.data['annotations'] = instances['annotations']\n",
        "    print(self.data['refs'][0])\n",
        "    print(self.data['annotations'][0])\n",
        "    print(self.data['images'][0])\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.data['refs'][idx]['split'] == 'train':\n",
        "      self.training_data.append(self.data['refs'][idx])\n",
        "    else:\n",
        "      self.test_data.append(self.data['refs'][idx])\n",
        "    image = self.data['images'][idx]\n",
        "    '''if self.transform:      \n",
        "      image = self.transform(Image.open(image))'''\n",
        "\n",
        "    return self.training_data, self.test_data\n",
        "    #return self.data[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data['refs'])\n",
        " "
      ],
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_DataLoader:\n",
        "\n",
        "  data = {}\n",
        "\n",
        "  def __init__(self, data_dir='dataset/refcocog'):\n",
        "    self.root = root = os.path.expanduser(root+'images/') \n",
        "    f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "    self.data['refs'] = pickle.load(f)\n",
        "    instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "    instances = json.load(open(instances_file, 'r'))\n",
        "    self.data['images'] = instances['images']\n",
        "    self.data['annotations'] = instances['annotations']\n",
        "    print(self.data['refs'][0])\n",
        "    print(self.data['annotations'][0])\n",
        "    print(self.data['images'][0])\n",
        "\n",
        "  def get_datasets(datasets):\n",
        "    training_samples = []\n",
        "    training_data = []\n",
        "    test_data = []\n",
        "    print(datasets[0])\n",
        "    for i,v in enumerate(datasets):\n",
        "      training_data, test_data = v\n",
        "      \n",
        "      '''if v['split'] == 'train':\n",
        "        training_data.append(v)\n",
        "      else:\n",
        "        test_data.append(v)'''\n",
        "\n",
        "    print(len(training_data))\n",
        "    print(len(test_data))  \n",
        "\n",
        "    return training_data, test_data\n",
        "\n",
        "\n",
        "    def preProcess_datasets(self, refs, anns, imgs):\n",
        "      images = []\n",
        "      texts = []       \n",
        "      for filename in imgs:\n",
        "        name = filename.stem\n",
        "        if name in refs:\n",
        "          for i, v in refs:\n",
        "            image = preprocess(Image.open(r'refcocog/images/'+v['file_name'])).unsqueeze(0).to(device)\n",
        "            for j, w in anns:\n",
        "              if(v['image_id'] == w['image_id']):\n",
        "                image = draw_bounding_boxes(image=image, boxes=w['bbox'], labels=v['sentences']['sent']).to(device)\n",
        "            image = torch.tensor(np.stack(image)).to(device)\n",
        "            text_token = clip.tokenize(v['sentences']['sent']).to(device)\n",
        "            image.shape, text_token.shape\n",
        "          images.append(image)\n",
        "          texts.append(text_token)\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "      return images, texts"
      ],
      "metadata": {
        "id": "PxhqoAtfkLaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = draw_bounding_boxes(image=image, boxes=w['bbox'], labels=v['sentences']['sent']).to(device)"
      ],
      "metadata": {
        "id": "2F7qkvPP4Lv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(root, batch_size=64, transform=False, test_batch_size=256):\n",
        "  \n",
        "    # build a chain of transformations\n",
        "    transformations_sequence = [\n",
        "      # random changes in pixel colors\n",
        "      T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "      # resize each PIL image to 256 x 256\n",
        "      T.Resize((256, 256)),                   \n",
        "      # the former transformations accept and return PIL Image objects, now convert to Tensor\n",
        "      T.ToTensor(),\n",
        "      # apply normalization\n",
        "      T.Normalize(mean=[0.4913, 0.4821, 0.4465], std=[0.2470, 0.2434, 0.2615])\n",
        "  ]\n",
        "    if not transform:\n",
        "      # convert the PIL images to Tensors\n",
        "      composed_transformation = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])  \n",
        "    else:\n",
        "      # prepare data transformations and then combine them sequentially\n",
        "      composed_transformation = T.Compose(transformations_sequence)\n",
        "\n",
        "    # load data\n",
        "    datasets = RefCOCOgDataset(root=root, transform=preprocess)\n",
        "\n",
        "  \n",
        "    full_training_data, test_data = Custom_DataLoader.get_datasets(datasets)\n",
        "\n",
        "    # create train and validation splits\n",
        "    num_samples = len(full_training_data)\n",
        "    training_samples = int(num_samples * 0.5 + 1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "\n",
        "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "    # initialize dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=4)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=4)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(images_fp: list[str], texts: list[str]):\n",
        "  # preprocess the images to transform from filenames to images to tensors\n",
        "  images = [preprocess(Image.open(image)) for image in images_fp]\n",
        "\n",
        "  # preprocess the texts to transform from text to tensors\n",
        "  images = torch.tensor(np.stack(images)).to(device)\n",
        "  text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "\n",
        "  # encode the inputs\n",
        "  with torch.no_grad():\n",
        "    images_z = model.encode_image(images).float().to(device)\n",
        "    texts_z = model.encode_text(text_tokens).float().to(device)\n",
        "  \n",
        "  return images_z, texts_z"
      ],
      "metadata": {
        "id": "didJ9Zi-F1dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (texts_z @ images_z.T)\n",
        "\n",
        "  return similarity.to(device)"
      ],
      "metadata": {
        "id": "TqNIHAq4F7aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "    # normalise the image and the text\n",
        "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # evaluate the cosine similarity between the sets of features\n",
        "    similarity = (texts_z @ images_z.T)\n",
        "    return similarity.to(device)"
      ],
      "metadata": {
        "id": "n7OqJi3m809h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to training mode\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "    \n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ],
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.classifier.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C)\n",
        "  Output: (N, C)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm1d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm1d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm1d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C) to (C, N)\n",
        "    x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # during training keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      # no computational graph is necessary to be built for this computation\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.transpose(0, 1)"
      ],
      "metadata": {
        "id": "d0_wL4DDev2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C, H, W)\n",
        "  Output: (N, C, H, W)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm2d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm2d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4, 5, 5)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm2d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C, H, W) to (C, N, H, W)\n",
        "    x = x.transpose(0, 1)\n",
        "    \n",
        "    # store the shape\n",
        "    c, bs, h, w = x.shape\n",
        "    \n",
        "    # collapse all dimensions except the 'channel' dimension\n",
        "    x = x.contiguous().view(c, -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.view(c, bs, h, w).transpose(0, 1)\n"
      ],
      "metadata": {
        "id": "RJfcWScgfZGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 10):\n",
        "    super().__init__()\n",
        "    model, _ = clip.load(\"RN50\")\n",
        "\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "\n",
        "    # add a linear layer\n",
        "    self.classifier = torch.nn.Linear(1024, num_classes)\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.encoder(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "QjBOBP-Wvha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_zero_shot_clip(net, data_loader, texts_z, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval()\n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      # these two lines are different from the \"traditional\" ones\n",
        "      images_z = model.encode_image(inputs).float()\n",
        "      outputs = (100 * images_z @ texts_z.T).softmax(dim=-1)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += inputs.shape[0]\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_accuracy / samples * 100"
      ],
      "metadata": {
        "id": "T7ydDaNEwR-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleneckCLIP(torch.nn.Module):\n",
        "  def __init__(self, bias=False):\n",
        "    super().__init__()\n",
        "    model, _ = clip.load(\"RN50\")\n",
        "    in_features = 1024\n",
        "    out_features = 1024\n",
        "\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "\n",
        "    # add a bottleneck\n",
        "    self.bottleneck = torch.nn.Sequential([\n",
        "      torch.nn.Linear(in_features, in_features // 2, bias=bias),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, in_features // 2, bias=bias),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, out_features, bias=bias),\n",
        "    ])\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.encoder(x)\n",
        "    x = self.bottleneck(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ],
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=65, \n",
        "         root='/content/dataset/refcocog/'):\n",
        "  \n",
        "  #writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "  # instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(root=root, batch_size=batch_size, transform=True, test_batch_size=None)\n",
        "\n",
        "  # pre-train model on zero-shot transfer learning\n",
        "  images, texts = map(list, zip(*[Custom_DataLoader.preProcess_datasets(Custom_DataLoader.data['refs'], Custom_DataLoader.data['annotations'], Custom_DataLoader.data['images'])])).to(device)\n",
        "  images_z, texts_z = encode_data(images, texts).to(device)\n",
        "  test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z).to(device)\n",
        "\n",
        "'''# evaluate accuracy on zero-shot learning\n",
        "  print(cosine_similarity(images_z, texts_z))\n",
        "  print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "  # instantiate the network and move it to the chosen device (GPU)\n",
        "  modified_model = CustomCLIP(num_classes=num_classes).to(device)\n",
        "\n",
        "  # instantiate the optimizer\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  # computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "    \n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')'''\n",
        "\n",
        "  # closes the logger\n",
        " # writer.close()"
      ],
      "metadata": {
        "id": "RmcsfGIf_1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3d86fdf5-51b3-44e5-9ed7-1dd34c18490d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# evaluate accuracy on zero-shot learning\\n  print(cosine_similarity(images_z, texts_z))\\n  print(\"Test accuracy {:.2f}\".format(test_accuracy))\\n\\n  # instantiate the network and move it to the chosen device (GPU)\\n  modified_model = CustomCLIP(num_classes=num_classes).to(device)\\n\\n  # instantiate the optimizer\\n  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\\n  \\n  # define the cost function\\n  cost_function = get_cost_function()\\n\\n  # computes evaluation results before training\\n  print(\\'Before training:\\')\\n  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\\n  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\\n  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\\n\\n  # log to TensorBoard\\n  log_values(writer, -1, train_loss, train_accuracy, \"train\")\\n  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\\n  log_values(writer, -1, test_loss, test_accuracy, \"test\")\\n\\n  print(\\'\\tTraining loss {:.5f}, Training accuracy {:.2f}\\'.format(train_loss, train_accuracy))\\n  print(\\'\\tValidation loss {:.5f}, Validation accuracy {:.2f}\\'.format(val_loss, val_accuracy))\\n  print(\\'\\tTest loss {:.5f}, Test accuracy {:.2f}\\'.format(test_loss, test_accuracy))\\n  print(\\'-----------------------------------------------------\\')\\n\\n  # for each epoch, train the network and then compute evaluation results\\n  for e in range(epochs):\\n    \\n    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\\n    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\\n\\n    # logs to TensorBoard\\n    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\\n\\n    print(\\'Epoch: {:d}\\'.format(e+1))\\n    print(\\'\\tTraining loss {:.5f}, Training accuracy {:.2f}\\'.format(train_loss, train_accuracy))\\n    print(\\'\\tValidation loss {:.5f}, Validation accuracy {:.2f}\\'.format(val_loss, val_accuracy))\\n    print(\\'-----------------------------------------------------\\')\\n\\n    # compute final evaluation results\\n    print(\\'After training:\\')\\n    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\\n    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\\n    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\\n\\n    # log to TensorBoard\\n    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\\n    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\\n    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\\n\\n    print(\\'\\tTraining loss {:.5f}, Training accuracy {:.2f}\\'.format(train_loss, train_accuracy))\\n    print(\\'\\tValidation loss {:.5f}, Validation accuracy {:.2f}\\'.format(val_loss, val_accuracy))\\n    print(\\'\\tTest loss {:.5f}, Test accuracy {:.2f}\\'.format(test_loss, test_accuracy))\\n    print(\\'-----------------------------------------------------\\')'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "CdKpBW-vPQfr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "91f94119-edd0-4913-c3ae-5a989d4b96f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-edbf329c711e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(batch_size, learning_rate, weight_decay, momentum, epochs, num_classes, root)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# pre-train model on zero-shot transfer learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreProcess_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'refs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mimages_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step_zero_shot_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'RefCOCOgDataset' has no attribute 'preProcess_datasets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "FW_hUAgX7kx2",
        "outputId": "21ee99ee-ee56-4003-91e8-d4c29fd887c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-f9252874d530>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNotebookApp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miopub_data_rate_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'NotebookApp' is not defined"
          ]
        }
      ]
    }
  ]
}