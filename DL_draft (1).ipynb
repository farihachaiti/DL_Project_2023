{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Prt7o-XfXak4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "1a4e3d8f-d7f6-434f-e5d9-ba9669bb5512"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtj2lvf3e2hL"
      },
      "outputs": [],
      "source": [
        "# !mkdir dataset\n",
        "# !pip install gdown\n",
        "# !gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "# !mv refcocog.tar.gz ./dataset/\n",
        "# !ls dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUre_MbJ4_p6"
      },
      "outputs": [],
      "source": [
        "# !gdown 1UmsNnuEs9kdstMs1t2NNmYhFZCC0C7Zq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgDGG1Li_69J"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "# data_dir='/content/dataset/refcocog.tar.gz'\n",
        "data_dir='/content/drive/MyDrive/refcocog.tar.gz'\n",
        "# Extract data\n",
        "tar = tarfile.open(data_dir)\n",
        "tar.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T34ktrdkz1k"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCCtvZcMWtFQ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1XfiBLCQJ4J"
      },
      "source": [
        "##Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-tTjnY4Y43Q"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f66FKDNVElbc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJXGH028L33X"
      },
      "source": [
        "All dependency or packages import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT6jx2_eL3B-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os, clip\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle, json\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAHC1TyGg8_8"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfxFmuwymwSE"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "                A.Resize(640, 640, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ])\n",
        "class RefCOCOgDataset(Dataset):\n",
        "  def __init__(self, transform = None):\n",
        "    super(RefCOCOgDataset, self).__init__()\n",
        "    self.images = []\n",
        "    self.descriptions = []\n",
        "    self.bboxes = []\n",
        "\n",
        "    f = open('refcocog/annotations/refs(umd).p', 'rb')\n",
        "    self.data = pickle.load(f)\n",
        "\n",
        "    instances_file = os.path.join('refcocog/annotations/instances.json')\n",
        "    instances = json.load(open(instances_file, 'r'))\n",
        "    self.insta = instances\n",
        "    cnt = 0\n",
        "    for item in self.data:\n",
        "      self.descriptions.append(item['sentences'])\n",
        "      cnt = cnt+1\n",
        "      for c in instances['images']:\n",
        "        if c['id'] == item['image_id']:\n",
        "            self.images.append(c['file_name'])\n",
        "            break\n",
        "      for c in instances['annotations']:\n",
        "        if c['id'] == item['ann_id']:\n",
        "            self.bboxes.append(c['bbox'])\n",
        "            break\n",
        "      if cnt == 100:\n",
        "        break\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = f'refcocog/images/{self.images[idx]}'\n",
        "    img_0 = cv2.imread(img_path)\n",
        "    # shape_0 = torch.tensor(list(img_0.shape))\n",
        "    # image = cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB)\n",
        "    # image = self.transform(image=img_0)['image']\n",
        "    # clip_image = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    labels = [x['raw'] for x in self.descriptions[idx]]\n",
        "    text_inputs = clip.tokenize(f\"a photo of a {labels[0]}\")\n",
        "    bbox = torch.tensor(self.bboxes[idx])\n",
        "    return img_0, text_inputs[0], bbox\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5z5a808u2yw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def calculate_iou(pred_boxes, target_boxes):\n",
        "\n",
        "    x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n",
        "    y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n",
        "    x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n",
        "    y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n",
        "\n",
        "    intersection = torch.clamp((x2 - x1 + 1), min=0) * torch.clamp((y2 - y1 + 1), min=0)\n",
        "\n",
        "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0] + 1) * (pred_boxes[:, 3] - pred_boxes[:, 1] + 1)\n",
        "    target_area = (target_boxes[:, 2] - target_boxes[:, 0] + 1) * (target_boxes[:, 3] - target_boxes[:, 1] + 1)\n",
        "    union = pred_area + target_area - intersection\n",
        "\n",
        "    iou = intersection / union\n",
        "\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEXHMP45kam0"
      },
      "outputs": [],
      "source": [
        "dataset = RefCOCOgDataset(transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8-_Wjd-USfm"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "to_pil = transforms.ToPILImage()\n",
        "def baseline(x):\n",
        "    image = x[0]\n",
        "    captions = x[1]\n",
        "    result = yolo(image)\n",
        "    bboxes = result.crop(save=False)\n",
        "    im_features = []\n",
        "    for bbox in bboxes:\n",
        "        image_input = preprocess(Image.fromarray(bbox['im'])).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image_input)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        im_features.append(image_features)\n",
        "    if len(bboxes) == 0:\n",
        "      return 0, torch.zeros(4)\n",
        "    text_inputs = captions.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_inputs)\n",
        "\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    im_features = torch.stack(im_features)\n",
        "    similarity = (100.0 * im_features @ text_features.T).softmax(dim=0)\n",
        "    top_values, top_indices = similarity.topk(1, dim=0)\n",
        "    return top_values.item(), bboxes[top_indices]['box']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou1d(box1, box2):\n",
        "    # Calculate coordinates of intersection rectangle\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # Compute area of intersection\n",
        "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "\n",
        "    # Compute areas of the two bounding boxes\n",
        "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "    # Compute IoU\n",
        "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
        "\n",
        "    return iou\n"
      ],
      "metadata": {
        "id": "CXrLuQTtM7xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_precision_recall(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    sorted_indices = np.argsort(pred_boxes[:, -1])[::-1]\n",
        "    pred_boxes = pred_boxes[sorted_indices]\n",
        "\n",
        "    num_predictions = len(pred_boxes)\n",
        "    num_targets = len(true_boxes)\n",
        "\n",
        "    true_positives = np.zeros(num_predictions)\n",
        "    false_positives = np.zeros(num_predictions)\n",
        "    false_negatives = np.zeros(num_targets)\n",
        "\n",
        "    for i, pred_box in enumerate(pred_boxes):\n",
        "        best_iou = 0\n",
        "        best_match_index = -1\n",
        "\n",
        "        for j, true_box in enumerate(true_boxes):\n",
        "            iou = calculate_iou1d(pred_box, true_box)\n",
        "\n",
        "            if iou > best_iou and iou >= iou_threshold:\n",
        "                best_iou = iou\n",
        "                best_match_index = j\n",
        "\n",
        "        if best_match_index >= 0:\n",
        "            true_positives[i] = 1\n",
        "            false_negatives[best_match_index] = 1\n",
        "        else:\n",
        "            false_positives[i] = 1\n",
        "\n",
        "    cum_true_positives = np.cumsum(true_positives)\n",
        "    cum_false_positives = np.cumsum(false_positives)\n",
        "    cum_false_negatives = np.cumsum(false_negatives)\n",
        "\n",
        "    precision = cum_true_positives / (cum_true_positives + cum_false_positives)\n",
        "    recall = cum_true_positives / (cum_true_positives + cum_false_negatives)\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "def calculate_ap(precision, recall):\n",
        "    recall = np.concatenate(([0], recall, [1]))\n",
        "    precision = np.concatenate(([0], precision, [0]))\n",
        "\n",
        "    for i in range(precision.size - 1, 0, -1):\n",
        "        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n",
        "\n",
        "    indices = np.where(recall[1:] != recall[:-1])[0] + 1\n",
        "    ap = np.sum((recall[indices] - recall[indices - 1]) * precision[indices])\n",
        "\n",
        "    return ap\n",
        "\n",
        "def calculate_mAP(ap_values):\n",
        "    mAP = np.mean(ap_values)\n",
        "    return mAP"
      ],
      "metadata": {
        "id": "mrdpbXp0MGTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Jxf117sxNm"
      },
      "outputs": [],
      "source": [
        "pred_boxes = []\n",
        "true_boxes = []\n",
        "\n",
        "for idx, x in enumerate(dataset):\n",
        "  print(f\"\\nTop predictions for {dataset.descriptions[idx][0]['raw']}:-------------------------\\n\")\n",
        "  conf, box = baseline(x)\n",
        "  print(\"target:\", x[2])\n",
        "  print(\"confidence: \", 100*conf)\n",
        "  print(\"box: \", box)\n",
        "  if conf != 0:\n",
        "    x1,y1, x2, y2 = box\n",
        "    pred_boxes.append([x1.item(), y1.item(), x2.item(), y2.item()])\n",
        "    true_boxes.append(x[2].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_boxes = np.array(pred_boxes)\n",
        "true_boxes = np.array(true_boxes)"
      ],
      "metadata": {
        "id": "Gq10dh2qOUaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall = calculate_precision_recall(pred_boxes, true_boxes)\n",
        "ap = calculate_ap(precision, recall)\n",
        "mAP = calculate_mAP(ap)\n",
        "\n",
        "print(\"AP values:\", ap)\n",
        "print(\"mAP:\", mAP)"
      ],
      "metadata": {
        "id": "gndMzkvAMg3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning starts here"
      ],
      "metadata": {
        "id": "k6IGJT1MS8da"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom dataset for finetuning"
      ],
      "metadata": {
        "id": "c8_8ihH4TmLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "                A.Resize(640, 640, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ])\n",
        "class RefCOCOgDataset(Dataset):\n",
        "  def __init__(self, transform = None):\n",
        "    super(RefCOCOgDataset, self).__init__()\n",
        "    self.images = []\n",
        "    self.descriptions = []\n",
        "    self.bboxes = []\n",
        "\n",
        "    f = open('refcocog/annotations/refs(umd).p', 'rb')\n",
        "    self.data = pickle.load(f)\n",
        "\n",
        "    instances_file = os.path.join('refcocog/annotations/instances.json')\n",
        "    instances = json.load(open(instances_file, 'r'))\n",
        "    self.insta = instances\n",
        "    cnt = 0\n",
        "    for item in self.data:\n",
        "      self.descriptions.append(item['sentences'])\n",
        "      cnt = cnt+1\n",
        "      for c in instances['images']:\n",
        "        if c['id'] == item['image_id']:\n",
        "            self.images.append(c['file_name'])\n",
        "            break\n",
        "      for c in instances['annotations']:\n",
        "        if c['id'] == item['ann_id']:\n",
        "            self.bboxes.append(c['bbox'])\n",
        "            break\n",
        "      if cnt == 100:\n",
        "        break\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img_path = f'refcocog/images/{self.images[idx]}'\n",
        "    img_0 = cv2.imread(img_path)\n",
        "    shape_0 = torch.tensor(list(img_0.shape))\n",
        "    image = cv2.cvtColor(img_0, cv2.COLOR_BGR2RGB)\n",
        "    image = self.transform(image=image)['image']\n",
        "    clip_image = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    labels = [x['raw'] for x in self.descriptions[idx]]\n",
        "    text_inputs = clip.tokenize(f\"a photo of a {labels[0]}\")\n",
        "    bbox = torch.tensor(self.bboxes[idx])\n",
        "    return clip_image, text_inputs[0], bbox\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "metadata": {
        "id": "Bjr_r5kJTR22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NDcSBWOOGiF"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size=15, test_batch_size=15):\n",
        "  full_training_data = RefCOCOgDataset(transform=transform)\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples * 0.5 + 1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=2)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=2)\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxuT8DbpNHy1"
      },
      "outputs": [],
      "source": [
        "da = get_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare model"
      ],
      "metadata": {
        "id": "UQC2gE9HTiBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxNHiX-PEeUE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from transformers import ViTModel\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ViTBBModel(nn.Module):\n",
        "    def __init__(self, num_boxes, hidden_dim=512):\n",
        "        super(ViTBBModel, self).__init__()\n",
        "\n",
        "        self.backbone = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "        self.ln = nn.Linear(self.backbone.config.hidden_size, hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.ln2 = nn.Linear(hidden_dim, 4 * num_boxes)\n",
        "\n",
        "        self.mlp_regressor = nn.Sequential(\n",
        "            nn.Linear(self.backbone.config.hidden_size, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, 4 * num_boxes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x).last_hidden_state\n",
        "        f = self.ln(features[:,0])\n",
        "        f = self.relu(f)\n",
        "        f = self.ln2(f)\n",
        "        return f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "metadata": {
        "id": "yL47yC1UWccT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, _ = clip.load(\"RN50\")"
      ],
      "metadata": {
        "id": "jjbyF0uAWezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = RefCOCOgDataset(transform=transform)"
      ],
      "metadata": {
        "id": "9LjuFEBvct9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def non_max_suppression(yolo_output, confidence_threshold, iou_threshold):\n",
        "    # Extract bounding box coordinates, class predictions, and confidence scores from the YOLO output\n",
        "    boxes = yolo_output[..., :4]\n",
        "    class_predictions = yolo_output[..., 5:]\n",
        "    confidence_scores = yolo_output[..., 4]\n",
        "\n",
        "    selected_boxes = []\n",
        "    for box, class_probs, confidence_scores in zip(boxes, class_predictions, confidence_scores):\n",
        "        # Apply confidence thresholding\n",
        "        mask = confidence_scores >= confidence_threshold\n",
        "        box = box[mask].cpu()\n",
        "        class_probs = class_probs[mask].cpu()\n",
        "        confidence_scores = confidence_scores[mask].cpu()\n",
        "\n",
        "        # Apply non-maximum suppression\n",
        "        selected_indices = []\n",
        "        while len(confidence_scores) > 0:\n",
        "            # Select the box with the highest confidence score\n",
        "            best_index = np.argmax(confidence_scores)\n",
        "            selected_indices.append(best_index)\n",
        "\n",
        "            # Calculate IoU between the best box and the rest\n",
        "            ious = calculate_iou(box[best_index], box[selected_indices[:-1]])\n",
        "\n",
        "            # Remove indices of boxes with IoU greater than the threshold\n",
        "            indices_to_remove = [i for i, iou in enumerate(ious) if iou > iou_threshold]\n",
        "            box = np.delete(box, indices_to_remove, axis=0)\n",
        "            class_probs = np.delete(class_probs, indices_to_remove, axis=0)\n",
        "            confidence_scores = np.delete(confidence_scores.cpu(), indices_to_remove, axis=0)\n",
        "\n",
        "        # Collect the selected boxes along with class IDs and confidence scores\n",
        "        selected_boxes.extend([(b[0], b[1], b[2], b[3], np.argmax(c), s) for b, c, s in zip(box, class_probs, confidence_scores)])\n",
        "\n",
        "    return selected_boxes\n",
        "\n",
        "def calculate_iou(box1, boxes2):\n",
        "    x1 = np.maximum(box1[0], boxes2[0])\n",
        "    y1 = np.maximum(box1[1], boxes2[1])\n",
        "    x2 = np.minimum(box1[2], boxes2[2])\n",
        "    y2 = np.minimum(box1[3], boxes2[3])\n",
        "\n",
        "    intersection = np.maximum(0, x2 - x1 + 1) * np.maximum(0, y2 - y1 + 1)\n",
        "    area_box1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    area_boxes2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n",
        "    union = area_box1 + area_boxes2 - intersection\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou\n"
      ],
      "metadata": {
        "id": "lWxL54fhh_vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPPB2wFZ7nSO"
      },
      "outputs": [],
      "source": [
        "# from yolov5.utils.general import (non_max_suppression, scale_boxes)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomClip(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.detector = model\n",
        "        self.clip_model = clip_model\n",
        "    def givemebox(self, img, bbox):\n",
        "      x1 = int(bbox[0])\n",
        "      y1 = int(bbox[1])\n",
        "      x2 = int(bbox[2])\n",
        "      y2 = int(bbox[3])\n",
        "      t = torch.stack((img[0][0][y1:y2, x1:x2], img[0][1][y1:y2, x1:x2], img[0][2][y1:y2, x1:x2]))\n",
        "      t = to_pil(t)\n",
        "      t = preprocess(t).unsqueeze(0).to(device)\n",
        "      return t\n",
        "    def forward(self, x):\n",
        "        similar = []\n",
        "        clip_image = x[0].to(device)\n",
        "        captions = x[1].to(device)\n",
        "        im_features = []\n",
        "        results = self.detector(clip_image, size=640)\n",
        "        results = non_max_suppression(results, 0.2, 0.2)\n",
        "        for det in results:\n",
        "          for *xyxy, conf, cls in det:\n",
        "            try:\n",
        "              t = self.givemebox(clip_image, xyxy)\n",
        "            except Exception as e:\n",
        "              continue\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.encode_image(t)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            im_features.append(image_features)\n",
        "        im_features = torch.stack(im_features)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          text_features = self.clip_model.encode_text(captions)\n",
        "          text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        print(im_features.shape)\n",
        "        similarity = (100.0 * im_features @ text_features.T).softmax(dim=0)\n",
        "        top_values, top_indices = similarity.topk(1, dim=0)\n",
        "        print(top_indices)\n",
        "        print(len(results))\n",
        "        return results[top_indices]\n",
        "\n",
        "x = test_dataset[0]\n",
        "\n",
        "net = CustomClip()\n",
        "res = net((x[0].unsqueeze(0), x[1].unsqueeze(0)))\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag-CVvdzvIDN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class IoULoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred_boxes, target_boxes):\n",
        "        batch_size, num_boxes = pred_boxes.size()\n",
        "\n",
        "        iou = calculate_iou(pred_boxes, target_boxes)\n",
        "        loss = 1 - iou\n",
        "        return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glR14_FUW3ds"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  loss = IoULoss()\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VCNbFs5W24g"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WufJEqd1WxXH"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  # set the network to training mode\n",
        "  cumulative_cIoU = 0.0\n",
        "  cumulative_count = 0.0\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (img, label, box) in enumerate(data_loader):\n",
        "      inputs = img.to(device)\n",
        "      box = box.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = net((inputs, label))\n",
        "      loss = cost_function(outputs, box)\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss\n",
        "      box[batch_idx].to(device)\n",
        "      cIoU = calculate_iou(outputs, box)\n",
        "      cumulative_cIoU += cIoU.sum().item()\n",
        "      cumulative_count += len(outputs)\n",
        "      # print(cumulative_cIoU)\n",
        "      # print(cumulative_count)\n",
        "      torch.cuda.empty_cache()\n",
        "  return cumulative_loss / samples, cumulative_cIoU / cumulative_count\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_cIoU = 0.0\n",
        "  cumulative_count = 0.0\n",
        "\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (img, label, box) in enumerate(data_loader):\n",
        "      inputs = img.to(device)\n",
        "      box = box.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = net((inputs, label))\n",
        "      loss = cost_function(outputs, box)\n",
        "\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss\n",
        "\n",
        "      cIoU = calculate_iou(outputs, box)\n",
        "      cumulative_cIoU += cIoU.sum().item()\n",
        "      cumulative_count += len(outputs)\n",
        "      # print(cumulative_cIoU)\n",
        "      # print(cumulative_count)\n",
        "      torch.cuda.empty_cache()\n",
        "  return cumulative_loss / samples, cumulative_cIoU / cumulative_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--OGJna-XwiA"
      },
      "outputs": [],
      "source": [
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4ygfFUkRHkk"
      },
      "outputs": [],
      "source": [
        "def main(batch_size=15,\n",
        "         learning_rate=0.001,\n",
        "         weight_decay=0.000001,\n",
        "         momentum=0.9,\n",
        "         epochs=20):\n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "  modified_model = CustomClip().to(device)\n",
        "  # modified_model = ViTBBModel(num_boxes=1).to(device)\n",
        "  # modified_model = AttentionClip().to(device)\n",
        "  train_loader, val_loader = da\n",
        "\n",
        "  # test_labels = torch.tensor([item[2] for item in test_dataset])\n",
        "  # test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
        "  # test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  # val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  # test_loss, test_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  # log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  # log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  # print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  # print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "\n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    # val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    # log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    # print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  # compute final evaluation results\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  # val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  # test_loss, test_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "  # log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "  # log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  # print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  # print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpYy55BrXsq1"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZwGRYiXk4KR"
      },
      "outputs": [],
      "source": [
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiS8DlsXU_hg"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}