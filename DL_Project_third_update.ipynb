{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q8rJi84feX2",
        "outputId": "1bdc712e-897d-482a-d4ba-2977c2758132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [01:52<00:00, 120MB/s]\n",
            "refcocog.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!mkdir dataset\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!mv refcocog.tar.gz ./dataset/\n",
        "!ls dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2vcCtcBfh9k",
        "outputId": "75e8a436-ec3f-49f3-c4a9-b865c1b6bee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "refcocog  refcocog.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!tar -xf dataset/refcocog.tar.gz -C dataset\n",
        "!ls dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LdnLcbFyfkvW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "from posixpath import split\n",
        "import json\n",
        "import tarfile\n",
        "import io\n",
        "import pickle\n",
        "import sys\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import torch\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZum46_RLQ44",
        "outputId": "9cd42b75-b4b3-4de8-a100-a07f9dcdeef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-isku007i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-isku007i\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=ba50e0c18a5bb0259b3e7d8d6a26c907fef66fa7f3e78dfb30d849c81cbc3677\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v2yj2yp6/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4udizeKooQS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe9ae10-8412-454b-fb02-ee2f46c43bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 244M/244M [00:07<00:00, 34.8MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from clip import clip\n",
        "\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "model = model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2b4mMLT8vbdE"
      },
      "outputs": [],
      "source": [
        "class RefCOCOgDataset(Dataset):\n",
        "  def __init__(self, dataset, transform=None, data_dir='dataset/refcocog'):\n",
        "    super(RefCOCOgDataset, self).__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.transform = transform\n",
        "    self.dataset = dataset\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):      \n",
        "    data_item = {} \n",
        "    if idx in self.dataset:\n",
        "      name = self.dataset[idx]['image']\n",
        "      #name = 'COCO_train2014_000000007307.jpg'     \n",
        "      fname = os.path.join(self.data_dir+'/images/', name)\n",
        "      if os.path.exists(fname):\n",
        "        img = Image.open(fname).convert('RGB')   \n",
        "        image = self.transform(img)['image']\n",
        "        data_item[idx]['image'] = image.permute(2, 0, 1).float()\n",
        "        data_item[idx]['texts'] = self.dataset[idx]['texts']\n",
        "        print(data_item[idx])\n",
        "    \n",
        "    return data_item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PxhqoAtfkLaI"
      },
      "outputs": [],
      "source": [
        "def get_datasets(refs, train = True):\n",
        "  ref_data = []\n",
        "  for val in refs:\n",
        "    if train:\n",
        "      if val['split'] == 'train' or val['split'] == 'val':\n",
        "        ref_data.append(val)\n",
        "      elif val['split'] == 'test':\n",
        "        ref_data.append(val)\n",
        "  \n",
        "  '''for i,v in enumerate(datasets):\n",
        "    training_data, test_data = v'''\n",
        "\n",
        "  return ref_data\n",
        "\n",
        "\n",
        "def preProcess_datasets(data_dir, train=True ):\n",
        "  dataset = defaultdict(dict)\n",
        "  data = {}\n",
        "  refs = {}     \n",
        "  f = open(f'{data_dir}/annotations/refs(umd).p', 'rb')\n",
        "  data['refs'] = pickle.load(f)\n",
        "  if(train):\n",
        "    refs = get_datasets(data['refs'], train=True)\n",
        "  else:\n",
        "    refs = get_datasets(data['refs'], train=False)\n",
        "  instances_file = os.path.join(f'{data_dir}/annotations/instances.json')\n",
        "  instances = json.load(open(instances_file, 'r'))\n",
        "  data['images'] = instances['images']\n",
        "  data['annotations'] = instances['annotations'] \n",
        "  print(data['refs'][0])\n",
        "  print(data['images'][0])\n",
        "  print(data['annotations'][0])\n",
        "  for idx in data['images']:\n",
        "    for v in refs:\n",
        "      if(v['image_id'] == idx['id']): \n",
        "        for w in data['annotations']:\n",
        "          if(v['image_id'] == w['image_id']):\n",
        "            for inner_i, inner_v in enumerate(v['sentences']):\n",
        "              dataset[w['image_id']]['image'] = v['file_name']\n",
        "              dataset[w['image_id']]['texts'] = {}\n",
        "              dataset[w['image_id']]['texts'][inner_i] = inner_v['sent']\n",
        "          else: \n",
        "            continue\n",
        "            \n",
        "  return dataset\n",
        "\n",
        "  def visualise_result(img, txt):\n",
        "    resize_image = T.Resize(100)\n",
        "    img = resize_image(img)\n",
        "    convert_tensor = T.ToTensor()\n",
        "    image = convert_tensor(img).to(device)\n",
        "    image = image.clone().detach()\n",
        "    image = image.type(torch.ByteTensor).to(device)\n",
        "    boxs = torch.tensor(w['bbox'],dtype=torch.int).to(device) \n",
        "    boxes = boxs.reshape([1,4])   \n",
        "    boxes = torchvision.ops.box_convert(boxes, \"cxcywh\", \"xyxy\").to(device)\n",
        "    image = draw_bounding_boxes(image=image, boxes=boxes, width=2, colors=(0,0,255), fill=True).to(device)\n",
        "    image = image.permute(1,2,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "didJ9Zi-F1dN"
      },
      "outputs": [],
      "source": [
        "def encode_data(images_fp: list[str], texts: list[str]):\n",
        "  # preprocess the images to transform from filenames to images to tensors\n",
        "  images = [preprocess(Image.open(image)) for image in images_fp]\n",
        "\n",
        "  # preprocess the texts to transform from text to tensors\n",
        "  images = torch.tensor(np.stack(images_fp)).to(device)\n",
        "  text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).to(device)\n",
        "  images.shape, text_tokens.shape\n",
        "  # encode the inputs\n",
        "  with torch.no_grad():\n",
        "    images_z = model.encode_image(images).float().to(device)\n",
        "    texts_z = model.encode_text(text_tokens).float().to(device)\n",
        "  \n",
        "  return images_z, texts_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TqNIHAq4F7aM"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (texts_z @ images_z.T)\n",
        "\n",
        "  return similarity.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TIeelz-AF32o"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to training mode\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "    \n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_loss = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nB6aV1uyGAxd"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XFYNX6i_GA-O"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': model.classifier.parameters(), 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d0_wL4DDev2a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 1D input (or 2D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C)\n",
        "  Output: (N, C)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm1d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm1d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm1d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C) to (C, N)\n",
        "    x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # during training keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      # no computational graph is necessary to be built for this computation\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RJfcWScgfZGR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Applies Batch Normalization over a 2D or 3D input (4D tensor)\n",
        "\n",
        "Shape:\n",
        "  Input: (N, C, H, W)\n",
        "  Output: (N, C, H, W)\n",
        "\n",
        "Input Parameters:\n",
        "  in_features: number of features of the input activations\n",
        "  track_running_stats: whether to keep track of running mean and std. (default: True)\n",
        "  affine: whether to scale and shift the normalized activations. (default: True)\n",
        "  momentum: the momentum value for the moving average. (default: 0.9)\n",
        "\n",
        "Usage:\n",
        "  >>> # with learable parameters\n",
        "  >>> bn = BatchNorm2d(4)\n",
        "  >>> # without learable parameters\n",
        "  >>> bn = BatchNorm2d(4, affine=False)\n",
        "  >>> input = torch.rand(10, 4, 5, 5)\n",
        "  >>> out = bn(input)\n",
        "\"\"\"\n",
        "\n",
        "class BatchNorm2d(torch.nn.Module):\n",
        "  def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.in_features = in_features\n",
        "    self.track_running_stats = track_running_stats\n",
        "    self.affine = affine\n",
        "    self.momentum = momentum\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = torch.nn.Parameter(torch.ones(self.in_features, 1))\n",
        "      self.beta = torch.nn.Parameter(torch.zeros(self.in_features, 1))\n",
        "    \n",
        "    if self.track_running_stats:\n",
        "      # register_buffer registers a tensor as a buffer that will be saved as part of the model\n",
        "      # but which does not require to be trained, differently from nn.Parameter\n",
        "      self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n",
        "      self.register_buffer('running_std', torch.ones(self.in_features, 1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transpose (N, C, H, W) to (C, N, H, W)\n",
        "    x = x.transpose(0, 1)\n",
        "    \n",
        "    # store the shape\n",
        "    c, bs, h, w = x.shape\n",
        "    \n",
        "    # collapse all dimensions except the 'channel' dimension\n",
        "    x = x.contiguous().view(c, -1)\n",
        "    \n",
        "    # calculate batch mean\n",
        "    mean = x.mean(dim=1).view(-1, 1)\n",
        "    \n",
        "    # calculate batch std\n",
        "    std = x.std(dim=1).view(-1, 1)\n",
        "    \n",
        "    # keep running statistics (moving average of mean and std)\n",
        "    if self.training and self.track_running_stats:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "        self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n",
        "    \n",
        "    # during inference time\n",
        "    if not self.training and self.track_running_stats:\n",
        "      mean = self.running_mean\n",
        "      std = self.running_std\n",
        "    \n",
        "    # normalize the input activations\n",
        "    x = (x - mean) / std\n",
        "    \n",
        "    # scale and shift the normalized activations\n",
        "    if self.affine:\n",
        "      x = x * self.gamma + self.beta\n",
        "    \n",
        "    return x.view(c, bs, h, w).transpose(0, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T7ydDaNEwR-2"
      },
      "outputs": [],
      "source": [
        "def test_step_zero_shot_clip(net, data_loader, texts_z, device='cuda'):\n",
        "  samples = 0.0\n",
        "  cumulative_accuracy = 0.0\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval()\n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      # these two lines are different from the \"traditional\" ones\n",
        "      images_z = model.encode_image(inputs).float()\n",
        "      outputs = (100 * images_z @ texts_z.T).softmax(dim=-1)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples += inputs.shape[0]\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t2f4f07KFLoF"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(torch.nn.Module):\n",
        "  def __init__(self, num_classes: int = 10, bias=False):\n",
        "    super().__init__()\n",
        "    model, _ = clip.load(\"RN50\")\n",
        "    in_features = 1024\n",
        "    out_features = 1024\n",
        "\n",
        "    # take the visual encoder of CLIP\n",
        "    # we also convert it to be 32 bit (by default CLIP is 16)\n",
        "    self.encoder = model.visual.float()\n",
        "\n",
        "    # add a bottleneck\n",
        "    self.bottleneck = torch.nn.Sequential([\n",
        "      torch.nn.Linear(in_features, in_features // 2, bias=bias),\n",
        "      torch.nn.BatchNorm1d(512),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, in_features // 2, bias=bias),\n",
        "      torch.nn.BatchNorm1d(512),\n",
        "      torch.nn.ReLU(inplace=True),\n",
        "      torch.nn.Linear(in_features // 2, out_features, bias=bias),\n",
        "      torch.nn.BatchNorm1d(1024),\n",
        "    ])\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.encoder(x)\n",
        "    x = self.bottleneck(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MGHhKD_Tgpjh"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir, batch_size=64, transform=False, test_batch_size=256):\n",
        "  images = []\n",
        "  texts = []\n",
        "  \n",
        "  if not transform:\n",
        "    # convert the PIL images to Tensors\n",
        "    transform = T.Compose([torchvision.transforms.ToTensor()]) \n",
        "  else:\n",
        "      # prepare data transformations and then combine them sequentially\n",
        "    transform = preprocess\n",
        "\n",
        "  # load data\n",
        "  full_training_data = preProcess_datasets(data_dir, train=True)\n",
        "  full_training_data = RefCOCOgDataset(full_training_data, transform=transform, data_dir=data_dir)\n",
        "  test_data = preProcess_datasets(data_dir, train=False)\n",
        "  test_data = RefCOCOgDataset(full_training_data, transform=transform, data_dir=data_dir)\n",
        "\n",
        "  # pre-train model on zero-shot transfer learning\n",
        "  for val in test_data:\n",
        "    images.append(val['image'])\n",
        "    texts.append(val['texts'])\n",
        "  images_z, texts_z = encode_data(images, texts).to(device)\n",
        "  \n",
        "\n",
        "  # evaluate accuracy on zero-shot learning\n",
        "  print(cosine_similarity(images_z, texts_z))\n",
        "  print(\"Test accuracy {:.2f}\".format(test_accuracy))\n",
        "\n",
        "  # create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples * 0.5 + 1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=2)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=2)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z).to(device)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V-TbnepNN8_u"
      },
      "outputs": [],
      "source": [
        "# tensorboard logging utilities\n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RmcsfGIf_1fd"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        " # main funcition\n",
        "def main(\n",
        "      root='/content/dataset/refcocog/',\n",
        "      data_dir='dataset/refcocog',\n",
        "      batch_size=32,\n",
        "      num_classes=10,\n",
        "      learning_rate=0.01,\n",
        "      weight_decay=0.000001,\n",
        "      momentum=0.9,\n",
        "      epochs=10,\n",
        "    ): \n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "  # train clip on zero-shot learning and instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(data_dir=data_dir, batch_size=batch_size, transform=True, test_batch_size=None)\n",
        "\n",
        "\n",
        "  # instantiate the network and move it to the chosen device (GPU)\n",
        "  modified_model = CustomCLIP(num_classes=num_classes).to(device)\n",
        "\n",
        "  # instantiate the optimizer\n",
        "  optimizer = get_optimizer(modified_model, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  # define the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  '''# evaluate accuracy of modified model on zero-shot learning\n",
        "  test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z).to(device)\n",
        "  \n",
        "  print(cosine_similarity(images_z, texts_z))\n",
        "  print(\"Test accuracy {:.2f}\".format(test_accuracy))'''\n",
        "\n",
        "  # computes evaluation results before training\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "  # log to TensorBoard\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "  log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "  print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # for each epoch, train the network and then compute evaluation results\n",
        "  for e in range(epochs):\n",
        "    \n",
        "    train_loss, train_accuracy = training_step(modified_model, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "\n",
        "    # logs to TensorBoard\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    # compute final evaluation results\n",
        "    print('After training:')\n",
        "    train_loss, train_accuracy = test_step(modified_model, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(modified_model, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(modified_model, test_loader, cost_function)\n",
        "\n",
        "    # log to TensorBoard\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  # closes the logger\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CdKpBW-vPQfr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "0ec82791-23e7-4a8d-97c2-775965fa139b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image_id': 380440, 'split': 'test', 'sentences': [{'tokens': ['the', 'man', 'in', 'yellow', 'coat'], 'raw': 'the man in yellow coat', 'sent_id': 8, 'sent': 'the man in yellow coat'}, {'tokens': ['skiier', 'in', 'red', 'pants'], 'raw': 'Skiier in red pants.', 'sent_id': 9, 'sent': 'skiier in red pants'}], 'file_name': 'COCO_train2014_000000380440_491042.jpg', 'category_id': 1, 'ann_id': 491042, 'sent_ids': [8, 9], 'ref_id': 0}\n",
            "{'license': 1, 'file_name': 'COCO_train2014_000000131074.jpg', 'coco_url': 'http://mscoco.org/images/131074', 'height': 428, 'width': 640, 'date_captured': '2013-11-21 01:03:06', 'flickr_url': 'http://farm9.staticflickr.com/8308/7908210548_33e532d119_z.jpg', 'id': 131074}\n",
            "{'segmentation': [[21.11, 239.09, 16.31, 274.6, 198.65, 349.45, 240.87, 336.98, 320.52, 293.79, 334.91, 248.69, 357.95, 273.64, 353.15, 289.0, 398.25, 267.88, 437.6, 251.57, 412.65, 228.54, 240.87, 210.31, 219.76, 141.21, 113.24, 153.69, 63.34, 156.57, 26.87, 169.04]], 'area': 48667.84089999999, 'iscrowd': 0, 'image_id': 131074, 'bbox': [16.31, 141.21, 421.29, 208.24], 'category_id': 65, 'id': 318235}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-29c3c97c199f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(root, data_dir, batch_size, num_classes, learning_rate, weight_decay, momentum, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# train clip on zero-shot learning and instantiates dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-4e6b950e97b2>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(data_dir, batch_size, transform, test_batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mfull_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcess_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mfull_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRefCOCOgDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_training_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcess_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-c3a10bcbf02a>\u001b[0m in \u001b[0;36mpreProcess_datasets\u001b[0;34m(data_dir, train)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m           \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minner_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_v\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}